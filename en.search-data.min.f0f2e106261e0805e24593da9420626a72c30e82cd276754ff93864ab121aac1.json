[{"id":0,"href":"/middleware/docs/mysql/advanced/ACID/isolution/","title":"隔离性-锁","section":"ACID","content":" 锁原理 # 首先，如果要你实现一把锁，该如何实现呢？就从简单到复杂,慢慢考虑性能等一些因素。\n一个变量 一个抢占动作: 抢占失败(比如已经被其他线程抢占了)处理: immediately-retry yield sleep park mix(immediately-retry, yield/sleep/park) 重试 下面是它的伪代码:\nvar v = 0 func unlock(){ //解锁 v=0 } func lock(){ //加锁 retry: if !CompareAndSet(0, 1){ handleErr() goto retry } } func handleErr(){ // 处理方式有如下几种 // immediately-retry // yield // sleep // park // mix(immediately-retry, yield/sleep/park) } func CompareAndSet(except, newValue int)bool{ //cas操作,如果修改v则返回true } 下面我们来讲讲这几种CAS失败后处理方式。\n自旋 # 最容易想到可能是自旋：\nfunc handleErr(){ // 可以使用CPU提供的PAUSE指令来实现「忙等待」，因为可以减少循环等待时的耗电量 return // 直接返回，马上重试// immediately-retry } 这样实现的锁显然有个致命的缺点：耗费cpu资源。没有竞争到锁的线程会一直占用cpu资源进行cas操作，假如一个线程获得锁后要花费10s处理业务逻辑，那另外一个线程就会白白的花费10s的cpu资源。（假设系统中就只有这两个线程的情况）。\nyield # 要解决自旋锁的性能问题必须让竞争锁失败的线程不忙等,而是在获取不到锁的时候能把cpu资源给让出来，说到让cpu资源，你可能想到了yield()方法，看看下面的例子：\nfunc handleErr(){ return yield() // yield } 当线程竞争锁失败时，会调用yield方法让出cpu。需要注意的是该方法只是当前让出cpu，有可能操作系统下次还是选择运行该线程。其实现是 将当期线程移动到所在优先调度队列的末端（操作系统线程调度了解一下？有时间的话，下次写写这块内容）。也就是说，如果该线程处于优先级最高的调度队列且该队列只有该线程，那操作系统下次还是运行该线程。\n自旋+yield的方式并没有完全解决问题，当系统只有两个线程竞争锁时，yield是有效的。但是如果有100个线程竞争锁，当线程1获得锁后，还有99个线程在反复的自旋+yield，线程2调用yield后，操作系统下次运行的可能是线程3；而线程3CAS失败后调用yield后，操作系统下次运行的可能是线程4\u0026hellip; 假如运行在单核cpu下，在竞争锁时最差只有1%的cpu利用率，导致获得锁的线程1一直被中断，执行实际业务代码时间变得更长，从而导致锁释放的时间变的更长。\nsleep # 你可能从一开始就想到了，当竞争锁失败后，可以将用Thread.sleep将线程休眠，从而不占用cpu资源：\nfunc handleErr(){ sleep(10) // sleep } 上述方式我们可能见的比较多，通常用于实现上层锁。该方式不适合用于操作系统级别的锁，因为作为一个底层锁，其sleep时间很难设置。sleep的时间取决于同步代码块的执行时间，sleep时间如果太短了，会导致线程切换频繁（极端情况和yield方式一样）；sleep时间如果设置的过长，会导致线程不能及时获得锁。因此没法设置一个通用的sleep值。就算sleep的值由调用者指定也不能完全解决问题：有的时候调用锁的人也不知道同步块代码会执行多久。\npark # 那可不可以在获取不到锁的时候让线程释放cpu资源进行等待，当持有锁的线程释放锁的时候将等待的线程唤起，然后再次进行竞争呢?\nQueue parkQueue; func handleErr(){ lock_wait() // park } func lock_wait(){ //将当期线程加入到等待队列 parkQueue.lpush(nowThread) //将当期线程释放cpu releaseCpu() } func unlock(){ lock_notify() } func lock_notify(){ //得到要唤醒的线程 Thread t=parkList.rpull() //唤醒等待线程 wakeAThread(t) } 上面是伪代码，描述这种设计思想，至于释放cpu资源、唤醒等待线程的的具体实现，后文会再说。这种方案相比于sleep而言，只有在锁被释放的时候，竞争锁的线程才会被唤醒，不会存在过早或过晚唤醒的问题。\n小结 # 当两个线程是属于同一个进程，因为虚拟内存是共享的，所以在切换时，虚拟内存这些资源就保持不动，只需要切换线程的私有数据、寄存器等不共享的数据。 所以线程切换是有代价的，如果当锁冲突不严重，用第一种方式(自旋锁)更适合，试想每个线程获得锁后很短的一段时间内就释放锁，竞争锁的线程只要经历几次自旋运算后就能获得锁，比上下文切换的开销少。\n现在的锁实现是比较复杂，至少会包含自旋+等待相结合的方式：在锁失败后会先自旋一定次数，如果还没获得锁再进行等待。(当然还有更加复杂的结合形式，比如golang里面的锁实现)\n死锁 # 在多个并发程序中，互相持有对方线程的独占锁 死锁的另一个常见来源是在goroutine中重复获取锁: // ----------------1----------------------------------- // 死锁最常见的来源之一是不一致的锁排序:比方说，您有两个互斥体A和B，在某些goroutines中: A.Lock() // defer A.Unlock() or similar. ... B.Lock() // defer B.Unlock() or similar. // 在其他goroutine中，获取锁的顺序是反的 B.Lock() // defer B.Unlock() or similar. ... A.Lock() // defer A.Unlock() or similar. // ----------------2----------------------------------- // 在goroutine中重复获取锁: A.Rlock() or lock() A.lock() or A.RLock() 互斥条件：一个资源每次只能被一个线程使用。\n请求与保持条件：一个线程因请求资源而阻塞时，对已获得的资源保持不放。\n不剥夺条件：线程已获得的资源，在未使用完之前，不能强行剥夺。\n循环等待条件：若干线程之间形成一种头尾相接的循环等待资源关系。\n互斥条件 \u0026mdash;\u0026gt; 独占锁的特点之一。\n请求与保持条件 \u0026mdash;\u0026gt; 独占锁的特点之一，尝试获取锁时并不会释放已经持有的锁\n不剥夺条件 \u0026mdash;\u0026gt; 独占锁的特点之一。\n循环等待条件 \u0026mdash;\u0026gt; 唯一需要记忆的造成死锁的条件。\n在并发程序中，避免了逻辑中出现复数个线程互相持有对方线程所需要的独占锁的的情况，就可以避免死锁。\n使用资源有序分配法;按照同一顺序来获取资源锁。\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;sync\u0026#34; ) import \u0026#34;time\u0026#34; var lockA sync.Mutex var lockB sync.Mutex func funcA(flag chan\u0026lt;- int) { lockA.Lock() time.Sleep(time.Second) lockB.Lock() lockB.Unlock() lockA.Unlock() flag \u0026lt;- 1 return } func funcB(flag chan\u0026lt;- int) { lockB.Lock() time.Sleep(time.Second) lockA.Lock() lockA.Unlock() lockB.Unlock() flag \u0026lt;- 1 return } func main() { var ( tick = time.NewTicker(10 * time.Second) flag = make(chan int, 2) flagNum int ) go funcA(flag) go funcB(flag) for { select { case \u0026lt;-tick.C: fmt.Println(\u0026#34;overtime\u0026#34;) return case \u0026lt;-flag: if flagNum == 1 { fmt.Println(\u0026#34;all goroutine exist\u0026#34;) return } else { flagNum++ } } } } 读写锁 # 读锁和写锁都可以看作可以上锁的东西，只是写锁是排他性的，如果它上了锁，其他读/写锁就没办法加锁，而读锁它上锁后，其他读锁可以继续上锁，但排斥写锁。\n一般可分为三类:公平读写锁，读优先(可能导致写饥饿)，写优先(读饥饿)三类。\n按照写锁-\u0026gt;读锁的逻辑顺序去看:\n写锁加上了以后，是把v减去一个很大的值，使得它可以容纳一个很大的读锁。\nvar maxReadLockSize int32 = 1\u0026lt;\u0026lt;30 // ------------------------------ var ( lock LOCK // 前面所说的锁 v = int32(0) // 0 not lock; 1 write lock; 2 read lock; notifyWLockAfterReaderWait int32 = 0 ) func RLock(){ retry: if Add(\u0026amp;v, 1)\u0026lt;0{ // 写锁在占用 handleErr() goto retry } // 读抢到锁 } func RUnlock(){ if r := Add(\u0026amp;v, -1); r \u0026lt; 0 { if Add(\u0026amp;notifyWLockAfterReaderWait, -1) == 0{ // 所有的读完成了，应该去唤醒写 lock_notify() } } } func WLock(){ lock.Lock() // 先用前面的自旋/互斥锁锁住,同一时间只有一个写能进入这里面。 retry: r := Add(\u0026amp;v, -maxReadLockSize) + maxReadLockSize // 不等于0证明有正在读的动作 if r!=0 \u0026amp;\u0026amp; Add(\u0026amp;notifyWLockAfterReaderWait, r){ handleErr() goto retry } } func unlock(){ //解锁 v=0 } func lock(){ //加锁 } https://zhuanlan.zhihu.com/p/98443808 悲观与乐观锁 # 乐观锁和悲观锁是两种思想，它们的使用是非常广泛的，不局限于某种编程语言或数据库\n悲观是先一定要加上锁，然后继续执行其他逻辑 乐观是提交的时候才进行判断，是无锁的，而且这里提交与更新有可能是同一个动作，导致提交更新判断是原子性的。\n乐观锁 # CAS # 缺点是\nABA问题 假设有两个线程——线程1和线程2，两个线程按照顺序进行以下操作：\n(1)线程1读取内存中数据为A；\n(2)线程2将该数据修改为B；\n(3)线程2将该数据修改为A；\n(4)线程1对数据进行CAS操作\n在第(4)步中，由于内存中数据仍然为A，因此CAS操作成功，但实际上该数据已经被线程2修改过了。这就是ABA问题。\n在AtomicInteger的例子中，ABA似乎没有什么危害。但是在某些场景下，ABA却会带来隐患，例如栈顶问题：一个栈的栈顶经过两次(或多次)变化又恢复了原值，但是栈可能已发生了变化。\n对于ABA问题，比较有效的方案是引入版本号，内存中的值每发生一次变化，版本号都+1；在进行CAS操作时，不仅比较内存中的值，也会比较版本号，只有当二者都没有变化时，CAS才能执行成功。Java中的AtomicStampedReference类便是使用版本号来解决ABA问题的。\n只能应用于单值问题，CAS只能保证单个变量（或者说单个内存值）操作的原子性，这意味着：(1)原子性不一定能保证线程安全，例如在Java中需要与volatile配合来保证线程安全；(2)当涉及到多个变量(内存值)时，CAS也无能为力\n加版本号 # 玩家信息增加一个字段：version。在初次查询玩家信息时，同时查询出version信息；在执行update操作时，校验version是否发生了变化，如果version变化，则不进行更新。\npublic void updateCoins(Integer playerId){ //根据player_id查询玩家信息，包含version信息 Player player = query(\u0026#34;select coins, level, version from player where player_id = {0}\u0026#34;, playerId); //根据玩家当前信息及其他信息，计算新的金币数 Long newCoins = ……; //更新金币数，条件中增加对version的校验 update(\u0026#34;update player set coins = {0}, version = version + 1 where player_id = {1} and version = {2}\u0026#34;, newCoins, playerId, player.version); } 可见性,内存屏障与原子性， # 可见性:通常是指在cpu多级缓存下如何保证缓存的一致性，即在一个CPU上修改了了某个数据在其他的CPU上不会继续读取旧的数据， 内存屏障:通常是为了CPU为了提高流水线性能，而对指令进行重排序而来， 原子性:是指的执行某个操作的过程的不可分割\n# atomic/asm_amd64.s TEXT runtime∕internal∕atomic·Xadd(SB) LOCK # LOCK指令配合CPU的MESI协议，实现可见性和内存屏障 XADDL AX, 0(BX) # 通过XADDL则用来保证原子性 https://java366.com/blog/detail/409a3d2f952c92cbb84f4a1438b1dc0c mysql锁 # 一个是悲观锁（行锁/grap锁/next-key锁） 一个是乐观锁（mvcc使用undo log来实现的一致性非锁定读, 利用事务版本的概念来实现的锁）\n悲观锁 # 乐观锁 # ACID # 隔离性 # MySQL默认的隔离级别是RR\nIsolution\n四种隔离级别: 读未提交/读已提交/可重复读/串行\n读过程可能会发生的三种错误: 脏读/不可重复读/幻读\n\\ 脏读 不可重复读 幻读 串行 x x x 可重复读 x x o 读已提交 x o o 读未提交 o o o 明确一点，如果当前有多个事物在写同一条记录，会存在竞争，也就是都是写的同一份记录所在的磁盘文件/内存。 MVCC的应用场景是在读的时候其他事物同时在写，不会影响我读的结果，也就是一致性非锁定读\n所以我们看下读未提交隔离级别，它本身就需要读到记录最新的结果，而串行隔离级别，每次只有一个事务在操作，不会出现读过程三种错误\n快照 # 创建ReadView(一致性读/快照)时机 # 读提交隔离级别中每个SQL开始时都会创建一个ReadView 可重复读级别中每个事物开始时都会创建一个ReadView\n事物开启的时机 # 一般有两种情况:\nbegin/start transaction; # 单独只执行此指令，事物不会真正发生,也不会创建ReadView select/udpate/insert/delete ... # 只有继续执行了增删查改操作的 SQL 语句，才是事务真正启动的时机 start transaction with consistent snapshot; // 马上就会开始事物，且创建ReadView 创建ReadView(一致性读/快照)逻辑 # 可重复读隔离级别: 当一个事务开启的时候，会向系统申请一个新事务id, 此时，可能还有多个正在进行的其他事务没有提交，因此在瞬时时刻，是有多个活跃的未提交事务id, 将这些未提交的事务id组成一个数组，数组里面最小的事务id记录为低水位，当前系统创建过的事务id的最大值记录为高水位, 这个数组array 和 高水位，就组成了“一致性视图”。\n可见性判断 # 未提及的事务 |_______________________________| | notCommitMinTxID notCommitMaxTxID systemMaxTxID 低水位 高水位 四个范围：\n可见：(0, notCommitMinTxID) 不可见：[notCommitMinTxID, notCommitMaxTxID] 可见：(notCommitMaxTxID, systemMaxTxID] 不可见：(systemMaxTxID, infine) innodb是否解决幻读问题 # 当前读：next-key lock（行锁+间隙锁）解决了 快照读：没有彻底解决，当快照读中混入当前读\n当前DB已有id 5, 10, 15三条数据。 事务A查询id \u0026lt; 10的数据，可以查出一行记录id = 5 事务B插入id = 6的数据 事务A再查询id \u0026lt; 10的数据，可以查出一行记录id = 5，查不出id = 6的数据（读场景，解决了幻读） 事务A可以更新/删除id = 6的数据，不能插入id = 6的数据（写场景，幻读不彻底） 这个很好理解，MySQL虽然通过MVCC的版本号来解决了读场景下的幻读，但对于上面第5步那种写场景的情况，其实是无能为力的，因为MVCC毕竟是无锁实现。\n所以如果后续要对数据进行写操作，还是用for update语句先上锁,进入当前读，加上next-key lock比较稳妥，不然就可能会出现上面第5步那样的问题。\nmysql隔离级别的可重复读本身不能解决幻读问题，但是现在mysql借助MVCC多版本控制和行锁解决了绝大部分的幻读问题。指出一下，第二个测试里事务B的序号 4 是可以查询出事务A所插入的数据吧。测试二中事务B出现幻读问题主要是，在事务A提交事务之后进行一次update操作，相当于select\u0026hellip;.for update，是当前读操作，会更新Read View，所以，当事务A提交之后，事务B进行update操作，读取的是最新版本的数据，也就是说事务A的操作对于事务B操作是可见性的。往往就是，RR级别下的幻读，是直接或间接的执行了当前读，导致Read View被更新，所以会读取最新版本的数据。个人觉得，mysql在RR级别下利用MCVV和行锁机制会出现幻读应该就是这个原因吧。\nhttps://www.yasinshaw.com/articles/125 原子性 # (atomic)\n使用undo log 来记录修改之前的旧记录, 是一个逻辑日志，不是物理的\n在对数据库进行修改时，innoDB引擎除了会产生redo log，还会产生undo log。InnoDB实现回滚，靠的是undo log：当事务对数据库进行修改时，InnoDB会生成对应的undo log；如果事务执行失败导致事务需要回滚，就利用undo log中的信息将数据回滚到修改之前的样子。\n有人认为undo log是redo log的逆过程，其实是不对的。两个日志文件其实都能看作是一种对数据的恢复操作，redo log恢复事务导致的数据页的修改，而undo log能够恢复数据记录到某个特定的版本。\n所以redo log是一种物理日志（数据页的修改），而undo log是一种逻辑日志（数据记录）。\nundo log还要另外一个重要作用，就是用于mvcc中，进行多版本控制，也就是实现事务隔离性的基础，当用户读取一行记录时，如果这个记录已接被其他事务占用，那么当前事务就可以通过undo读取之前的行版本信息，用来实现非锁定读取，就是“快照读”。\n持久性 # (duration)\nwrite ahead log WAL\n二阶段提交: 事务提交前，将 redo log 的写入拆成了两个步骤，prepare 和 commit。\n通常我们说 MySQL 的“双 1”配置，指的就是 sync_binlog 和 innodb_flush_log_at_trx_commit 都设置成 1。也就是说，一个事务完整提交前，需要等待两次刷盘，一次是 redo log（prepare 阶段），一次是 binlog。\nredo log是innodb的存储引擎产生的，而binlog是数据库的server层实现的。换句话说，如果你使用MySQL，换其他存储引擎，那么可能没有redo log，但是还是会有binlog。\n日志记录的内容形式不同。 binlog是一种逻辑日志，记录对应的SQL语句，而redo log记录了物理日志，是针对每个数据页的修改。\n日志写入磁盘时间不同。 binlog只有在事务提交后完成一次写入，对于一个事物而言，在binlog中只有一条记录。而redo log在事务进行中不断被写入，而且是并发写入的，不是顺序写入的。\n保存方式不同。 redo log 是循环写的，空间固定会用完；binlog 是可以追加写入的。“追加写”是指 binlog 文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。\ninner join \u0026lt;-\u0026gt; join\nThey are functionally equivalent, but INNER JOIN can be a bit clearer to read, especially if the query has other join types (i.e. LEFT or RIGHT or CROSS) included in it. 如何锁住一个范围\n5.7mysql的innodb的默认级别 是RR RC? 为什么RR能防止幻读，而RC不能防止幻读，由于间隙锁加行锁？next-key lock\n数据库事务ACID特性 数据库事务的4个特性： 原子性(Atomic): 事务中的多个操作，不可分割，要么都成功，要么都失败； All or Nothing.\n一致性(Consistency): 事务操作之后, 数据库所处的状态和业务规则是一致的; 比如a,b账户相互转账之后，总金额不变；\n隔离性(Isolation): 多个事务之间就像是串行执行一样，不相互影响;\n持久性(Durability): 事务提交后被持久化到永久存储.\nMySQL 中RC和RR隔离级别的区别 MySQL数据库中默认隔离级别为RR，但是实际情况是使用RC 和 RR隔离级别的都不少。好像淘宝、网易都是使用的 RC 隔离级别。那么在MySQL中 RC 和 RR有什么区别呢？我们该如何选择呢？为什么MySQL将RR作为默认的隔离级别呢？ 5.1 RC 与 RR 在锁方面的区别 1\u0026gt; 显然 RR 支持 gap lock(next-key lock)，而RC则没有gap lock。因为MySQL的RR需要gap lock来解决幻读问题。而RC隔离级别则是允许存在不可重复读和幻读的。所以RC的并发一般要好于RR；\n2\u0026gt; RC 隔离级别，通过 where 条件过滤之后，不符合条件的记录上的行锁，会释放掉(虽然这里破坏了“两阶段加锁原则”)；但是RR隔离级别，即使不符合where条件的记录，也不会是否行锁和gap lock；所以从锁方面来看，RC的并发应该要好于RR；另外 insert into t select \u0026hellip; from s where 语句在s表上的锁也是不一样的。\nI(Isolation)：https://zhuanlan.zhihu.com/p/118658549 锁+MVCC 快照读 当前读 Lock：https://zhuanlan.zhihu.com/p/109129926 ACD()：https://zhuanlan.zhihu.com/p/129860691 InnoDB存储引擎有三种行锁算法：\n行锁：在单行记录上的锁 间隙锁：Gap Lock，锁定一个范围，但不包含记录本身 Next-Key Lock:就是行锁+间隙锁，同时锁上一个范围，并且锁定记录本身 https://zhuanlan.zhihu.com/p/161933980\nhttps://www.cnblogs.com/kisun168/p/11320549.html\nmvcc 的实现 undolog 实践next-key Lock https://mp.weixin.qq.com/s/i5QWx3QPZNkV51ghFbtXCw\n锁 # what: InnoDB锁: 共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 排他锁（X）：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。 为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁：\n意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的 IX 锁。 （如果一个事务请求的锁模式与当前的锁兼容， InnoDB 就将请求的锁授予该事务； 反之， 如果两者不兼容，该事务就要等待锁释放。）\nwhy: 防止并发修改数据。 how: InnoDB加锁方法： 对于普通SELECT语句，InnoDB 不会加任何锁； 对于 UPDATE、 DELETE 和 INSERT 语句， InnoDB会自动给涉及数据集加排他锁（X)； 事务可以通过以下语句显式给记录集加共享锁或排他锁： 共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE。 其他 session 仍然可以查询记录，并也可以对该记录加 share mode 的共享锁。但是如果当前事务需要对该记录进行更新操作，则很有可能造成死锁。 排他锁（X)：SELECT * FROM table_name WHERE ... FOR UPDATE。其他 session 可以查询该记录，但是不能对该记录加共享锁或排他锁，而是等待获得锁 意向锁是 InnoDB 自动加的， 不需用户干预。 参考 # 乐观锁和悲观锁: https://www.cnblogs.com/kismetv/p/10787228.html "},{"id":1,"href":"/middleware/docs/mysql/init/","title":"开始篇","section":"Mysql","content":" MySQL逻辑架构 # 连接层 服务层 引擎层(存储引擎是基于表的，而不是数据库) 存储层 如何修改字符集 # 日志 # 查询日志 错误日志 二进制日志 "},{"id":2,"href":"/middleware/docs/mysql/business_facing/","title":"面向业务篇","section":"Mysql","content":" SQL执行顺序 # select # 5 ----\u0026gt; ... from # 1 ... where # 2 .... group by # 3 ... having # 4 ----\u0026gt; ... order by # 6 ... limit # 7 [offset] JOIN # 如何写出join语句\n慢SQL排查 # first:找到是那些sql慢. 慢查询的开启，设置阈值(如超过5秒钟的就是慢SQL)并捕获。 second:开始分析这些sql. explain + 慢SQL分析。 查询语句写的差。 关联 查询太多join（设计缺陷或者不得已的需求）。 索引失效：索引建了，但是没有用上。 show Profile查询SQL在MySQL数据库中的执行细节和生命周期情况。 mysql数据库服务器的参数调优。 "},{"id":3,"href":"/middleware/docs/mysql/advanced/innodb_index/","title":"索引","section":"高级篇","content":" 索引的分类 # 从分类入手，从底层索引的物理结构开始讨论构建索引的自己规范：\n按「数据结构」分类：B+tree索引、Hash索引、Full-text索引。 按「物理存储」分类：聚簇索引（主键索引）、二级索引（辅助索引）。 按「字段特性」分类：主键索引、唯一索引、普通索引、前缀索引。 按「字段个数」分类：单列索引、联合索引。\n索引建立标准 # 从业务：经常where/group by / order by 从物理结构: 字段区分度要大(最好是唯一性) 从物理结构: 非经常变更的字段，比如不要对电商项目的用户余额建立索引，因为索引字段频繁修改，由于要维护 B+Tree的有序性，那么就需要频繁的重建索引，这个过程是会影响数据库性能的。 自增类型 从业务 # 从物理结构 # // TODO 添加 索引底层页的物理结构 // TODO 添加 联合索引的物理结构\n首先创建索引和维护索引要耗费时间:\n占用物理空间，数量越多，占用空间越大； B+ 树为了维护索引有序性，都需要进行动态维护。 (聚簇)主键索引最好是自增的 # InnoDB 创建主键索引默认为聚簇索引，数据被存放在了 B+Tree 的叶子节点上。也就是说，同一个叶子节点内的各个数据是按主键顺序存放的，因此，每当有一条新的数据插入时，数据库会根据主键将其插入到对应的叶子节点中。\n如果我们使用自增主键，那么每次插入的新数据就会按顺序添加到当前索引节点的位置，不需要移动已有的数据，当页面写满，就会自动开辟一个新页面。因为每次插入一条新记录，都是追加操作，不需要重新移动数据，因此这种插入数据的方法效率非常高。\n如果我们使用非自增主键，由于每次插入主键的索引值都是随机的，因此每次插入新的数据时，就可能会插入到现有数据页中间的某个位置，这将不得不移动其它数据来满足新数据的插入，甚至需要从一个页面复制数据到另外一个页面，我们通常将这种情况称为页分裂。页分裂还有可能会造成大量的内存碎片，导致索引结构不紧凑，从而影响查询效率。\n举个例子，假设某个数据页中的数据是1、3、5、9，且数据页满了，现在准备插入一个数据7，则需要把数据页分割为两个数据页：\n(非聚簇)二级索引 # 区分度太小的不适合 # // 区分度太小的也不适合做索引，比如sex字段，它的值一般不超过三个，区分度太小，你就算找到了sex=man的，你还是需要在它剩下的里面查找b,c,这种相当于查找了三分之一个表。还需要回表。\n联合索引的顺序要挑选最常用的在最前面 # (a, b, c) 联合索引，是先按 a 排序，在 a 相同的情况再按 b 排序，在 b 相同的情况再按 c 排序。所以，b 和 c 是全局无序，局部相对有序的，这样在没有遵循最左匹配原则的情况下，是无法利用到索引的。 联合索引的最左匹配原则，在遇到范围查询（\u0026gt;、\u0026lt;、between、like 包括like \u0026lsquo;林%\u0026lsquo;这种）的时候，就会停止匹配，也就是范围列可以用到联合索引，但是范围列后面的列无法用到联合索引。 // 就知道了它对于联合索引的一些问题，比如a,b,c; where b,c的时候就不会进过索引。\n在 MySQL 5.6 之前，只能从 ID2 （主键值）开始一个个回表，到「主键索引」上找出数据行，再对比 b 字段值。 而 MySQL 5.6 引入的索引下推优化（index condition pushdown)， 可以在联合索引遍历过程中，对联合索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 索引最好设置为 NOT NULL # 为了更好的利用索引，索引列要设置为 NOT NULL 约束。有两个原因：\n第一原因：索引列存在 NULL 就会导致优化器在做索引选择的时候更加复杂，更加难以优化，因为可为 NULL 的列会使索引、索引统计和值比较都更复杂，比如进行索引统计时，count 会省略值为NULL 的行。\n第二个原因：NULL 值是一个没意义的值，但是它会占用物理空间，所以会带来的存储空间的问题，会导致更多的存储空间占用，因为 InnoDB 默认行存储格式COMPACT，会用 1 字节空间存储 NULL 值列表，如下图的黄色部分：\n索引 # what: A database index is a data structure that improves the speed of operations in a table\n不在索引列上做任何操作（计算、函数、(自动or手动)类型转换），会导致索引失效而转向全表扫描。 索引中范围条件右边的字段会全部失效。 MySQL在使用!=或者\u0026lt;\u0026gt;的时候无法使用索引会导致全表扫描。 is null、is not null也无法使用索引。 like以通配符开头%abc索引失效会变成全表扫描。 字符串不加单引号索引失效。 少用or，用它来连接时会索引失效。 "},{"id":4,"href":"/middleware/docs/mysql/advanced/function/","title":"聚合函数","section":"高级篇","content":" count函数 # count()是一个函数，他的形参可以是一个字段或表达式; 函数逻辑是统计符合查询条件的记录中，如果使当前字段/表达式不为null的个数。比如select count(1) from tmp_table; 那么就是统计tmp_table表有多少行，因它的每个记录都不会使得表达式1为null。返回是统计结果\ncount(*)==count(1) \u0026gt; count(聚簇索引) \u0026gt;= count(非聚簇索引) \u0026gt; count(非索引字段) count(*)等同于count(1) # InnoDB handles SELECT COUNT(*) and SELECT COUNT(1) operations in the same way. There is no performance difference.\n不需要读取记录中的字段值然后再去判断, 所以server层不需要innodb返回字段\ncount(聚簇索引) # 有辅助索引时，InnoDB循环遍历的对象就不是聚簇索引，而是辅助索引。\n如果有多个辅助索引, 优化器会使用key_len,挑选最小的辅助索引进行扫描。\n这是因为相同数量的辅助索引记录可以比聚簇索引记录占用更少的存储空间，所以辅助索引树比聚簇(主键)索引树小，这样遍历辅助索引的 I/O 成本比遍历聚簇(主键)索引的 I/O 成本小，因此「优化器」优先选择的是最小的辅助索引。\ncount(非聚簇索引) # 使用辅助索引来进行统计\n"},{"id":5,"href":"/middleware/docs/mysql/business_facing/sql/","title":"写好sql","section":"面向业务篇","content":" SQL # SQL执行顺序 # select # 5 ----\u0026gt; ... from # 1 ... where # 2 .... group by # 3 ... having # 4 ----\u0026gt; ... order by # 6 ... limit # 7 [offset] 七种JOIN # /* 1 */ SELECT \u0026lt;select_list\u0026gt; FROM TableA A LEFT JOIN TableB B ON A.Key = B.Key; /* 2 */ SELECT \u0026lt;select_list\u0026gt; FROM TableA A RIGHT JOIN TableB B ON A.Key = B.Key; /* 3 */ SELECT \u0026lt;select_list\u0026gt; FROM TableA A INNER JOIN TableB B ON A.Key = B.Key; /* 4 */ SELECT \u0026lt;select_list\u0026gt; FROM TableA A LEFT JOIN TableB B ON A.Key = B.Key WHERE B.Key IS NULL; /* 5 */ SELECT \u0026lt;select_list\u0026gt; FROM TableA A RIGHT JOIN TableB B ON A.Key = B.Key WHERE A.Key IS NULL; /* 6 */ SELECT \u0026lt;select_list\u0026gt; FROM TableA A FULL OUTER JOIN TableB B ON A.Key = B.Key; /* MySQL不支持FULL OUTER JOIN这种语法 可以改成 1+2 */ SELECT \u0026lt;select_list\u0026gt; FROM TableA A LEFT JOIN TableB B ON A.Key = B.Key UNION SELECT \u0026lt;select_list\u0026gt; FROM TableA A RIGHT JOIN TableB B ON A.Key = B.Key; /* 7 */ SELECT \u0026lt;select_list\u0026gt; FROM TableA A FULL OUTER JOIN TableB B ON A.Key = B.Key WHERE A.Key IS NULL OR B.Key IS NULL; /* MySQL不支持FULL OUTER JOIN这种语法 可以改成 4+5 */ SELECT \u0026lt;select_list\u0026gt; FROM TableA A LEFT JOIN TableB B ON A.Key = B.Key WHERE B.Key IS NULL; UNION SELECT \u0026lt;select_list\u0026gt; FROM TableA A RIGHT JOIN TableB B ON A.Key = B.Key WHERE A.Key IS NULL; 查询优化 # 小表驱动大表 # 优化原则：对于MySQL数据库而言，永远都是小表驱动大表。\n/** * 举个例子：可以使用嵌套的for循环来理解小表驱动大表。 * 以下两个循环结果都是一样的，但是对于MySQL来说不一样， * 第一种可以理解为，和MySQL建立5次连接每次查询1000次。 * 第一种可以理解为，和MySQL建立1000次连接每次查询5次。 */ for(int i = 1; i \u0026lt;= 5; i ++){ for(int j = 1; j \u0026lt;= 1000; j++){ } } // ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ for(int i = 1; i \u0026lt;= 1000; i ++){ for(int j = 1; j \u0026lt;= 5; j++){ } } IN和EXISTS\n/* 优化原则：小表驱动大表，即小的数据集驱动大的数据集 */ /* IN适合B表比A表数据小的情况*/ SELECT * FROM `A` WHERE `id` IN (SELECT `id` FROM `B`) /* EXISTS适合B表比A表数据大的情况 */ SELECT * FROM `A` WHERE EXISTS (SELECT 1 FROM `B` WHERE `B`.id = `A`.id); EXISTS：\n语法：SELECT....FROM tab WHERE EXISTS(subquery);该语法可以理解为： 该语法可以理解为：将主查询的数据，放到子查询中做条件验证，根据验证结果（true或是false）来决定主查询的数据结果是否得以保留。 提示：\nEXISTS(subquery)子查询只返回true或者false，因此子查询中的SELECT *可以是SELECT 1 OR SELECT X，它们并没有区别。 EXISTS(subquery)子查询的实际执行过程可能经过了优化而不是我们理解上的逐条对比，如果担心效率问题，可进行实际检验以确定是否有效率问题。 EXISTS(subquery)子查询往往也可以用条件表达式，其他子查询或者JOIN替代，何种最优需要具体问题具体分析。 ORDER BY优化 # 数据准备\nCREATE TABLE `talA`( `age` INT, `birth` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ); INSERT INTO `talA`(`age`) VALUES(18); INSERT INTO `talA`(`age`) VALUES(19); INSERT INTO `talA`(`age`) VALUES(20); INSERT INTO `talA`(`age`) VALUES(21); INSERT INTO `talA`(`age`) VALUES(22); INSERT INTO `talA`(`age`) VALUES(23); INSERT INTO `talA`(`age`) VALUES(24); INSERT INTO `talA`(`age`) VALUES(25); /* 创建索引 */ CREATE INDEX idx_talA_age_birth ON `talA`(`age`, `birth`); 案例\n/* 1.使用索引进行排序了 不会产生Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `age` \u0026gt; 20 ORDER BY `age`; /* 2.使用索引进行排序了 不会产生Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `age` \u0026gt; 20 ORDER BY `age`,`birth`; /* 3.没有使用索引进行排序 产生了Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `age` \u0026gt; 20 ORDER BY `birth`; /* 4.没有使用索引进行排序 产生了Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `age` \u0026gt; 20 ORDER BY `birth`,`age`; /* 5.没有使用索引进行排序 产生了Using filesort */ EXPLAIN SELECT * FROM `talA` ORDER BY `birth`; /* 6.没有使用索引进行排序 产生了Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `birth` \u0026gt; \u0026#39;2020-08-04 07:42:21\u0026#39; ORDER BY `birth`; /* 7.使用索引进行排序了 不会产生Using filesort */ EXPLAIN SELECT * FROM `talA` WHERE `birth` \u0026gt; \u0026#39;2020-08-04 07:42:21\u0026#39; ORDER BY `age`; /* 8.没有使用索引进行排序 产生了Using filesort */ EXPLAIN SELECT * FROM `talA` ORDER BY `age` ASC, `birth` DESC; ORDER BY子句，尽量使用索引排序，避免使用Using filesort排序。\nMySQL支持两种方式的排序，FileSort和Index，Index的效率高，它指MySQL扫描索引本身完成排序。FileSort方式效率较低。\nORDER BY满足两情况，会使用Index方式排序：\nORDER BY语句使用索引最左前列。 使用WHERE子句与ORDER BY子句条件列组合满足索引最左前列。 结论：尽可能在索引列上完成排序操作，遵照索引建的最佳左前缀原则。\n如果不在索引列上，File Sort有两种算法：MySQL就要启动双路排序算法和单路排序算法\n1、双路排序算法：MySQL4.1之前使用双路排序，字面意思就是两次扫描磁盘，最终得到数据，读取行指针和待ORDER BY列，対他们进行排序，然后扫描已经排序好的列表，按照列表中的值重新从列表中读取对应的数据输出。一句话，从磁盘取排序字段，在buffer中进行排序，再从磁盘取其他字段。\n取一批数据，要对磁盘进行两次扫描，众所周知，IO是很耗时的，所以在MySQL4.1之后，出现了改进的算法，就是单路排序算法。\n2、单路排序算法：从磁盘读取查询需要的所有列，按照ORDER BY列在buffer対它们进行排序，然后扫描排序后的列表进行输出，它的效率更快一些，避免了第二次读取数据。并且把随机IO变成了顺序IO，但是它会使用更多的空间，因为它把每一行都保存在内存中了。\n由于单路排序算法是后出的，总体而言效率好过双路排序算法。\n但是单路排序算法有问题：如果SortBuffer缓冲区太小，导致从磁盘中读取所有的列不能完全保存在SortBuffer缓冲区中，这时候单路复用算法就会出现问题，反而性能不如双路复用算法。\n单路复用算法的优化策略：\n增大sort_buffer_size参数的设置。 增大max_length_for_sort_data参数的设置。 提高ORDER BY排序的速度：\nORDER BY时使用SELECT *是大忌，查什么字段就写什么字段，这点非常重要。在这里的影响是：\n当查询的字段大小总和小于max_length_for_sort_data而且排序字段不是TEXT|BLOB类型时，会使用单路排序算法，否则使用多路排序算法。 两种排序算法的数据都有可能超出sort_buffer缓冲区的容量，超出之后，会创建tmp临时文件进行合并排序，导致多次IO，但是单路排序算法的风险会更大一些，所以要增大sort_buffer_size参数的设置。 尝试提高sort_buffer_size：不管使用哪种算法，提高这个参数都会提高效率，当然，要根据系统的能力去提高，因为这个参数是针对每个进程的。\n尝试提高max_length_for_sort_data：提高这个参数，会增加用单路排序算法的概率。但是如果设置的太高，数据总容量sort_buffer_size的概率就增大，明显症状是高的磁盘IO活动和低的处理器使用率。\nGORUP BY优化 # GROUP BY实质是先排序后进行分组，遵照索引建的最佳左前缀。\n当无法使用索引列时，会使用Using filesort进行排序，增大max_length_for_sort_data参数的设置和增大sort_buffer_size参数的设置，会提高性能。\nWHERE执行顺序高于HAVING，能写在WHERE限定条件里的就不要写在HAVING中了。\n总结 # 为排序使用索引\nMySQL两种排序方式：Using filesort和Index扫描有序索引排序。 MySQL能为排序与查询使用相同的索引，创建的索引既可以用于排序也可以用于查询。 /* 创建a b c三个字段的索引 */ idx_table_a_b_c(a, b, c) /* 1.ORDER BY 能使用索引最左前缀 */ ORDER BY a; ORDER BY a, b; ORDER BY a, b, c; ORDER BY a DESC, b DESC, c DESC; /* 2.如果WHERE子句中使用索引的最左前缀定义为常量，则ORDER BY能使用索引 */ WHERE a = \u0026#39;Ringo\u0026#39; ORDER BY b, c; WHERE a = \u0026#39;Ringo\u0026#39; AND b = \u0026#39;Tangs\u0026#39; ORDER BY c; WHERE a = \u0026#39;Ringo\u0026#39; AND b \u0026gt; 2000 ORDER BY b, c; /* 3.不能使用索引进行排序 */ ORDER BY a ASC, b DESC, c DESC; /* 排序不一致 */ WHERE g = const ORDER BY b, c; /* 丢失a字段索引 */ WHERE a = const ORDER BY c; /* 丢失b字段索引 */ WHERE a = const ORDER BY a, d; /* d字段不是索引的一部分 */ WHERE a IN (...) ORDER BY b, c; /* 对于排序来说，多个相等条件(a=1 or a=2)也是范围查询 */ SQL分析 # SQL性能下降的原因 # 查询语句写的差。 关联 查询太多join（设计缺陷或者不得已的需求）。 索引失效：索引建了，但是没有用上。 服务器调优以及各个参数的设置（缓冲、线程数等）。 EXPLAIN # EXPLAIN是什么？\nEXPLAIN：SQL的执行计划，使用EXPLAIN关键字可以模拟优化器执行SQL查询语句，从而知道MySQL是如何处理SQL语句的。\nEXPLAIN怎么使用？\n语法：explain + SQL。\nmysql\u0026gt; explain select * from pms_category \\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: pms_category partitions: NULL type: ALL possible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 1425 filtered: 100.00 Extra: NULL 1 row in set, 1 warning (0.00 sec) EXPLAIN能干嘛？\n可以查看以下信息：\nid：表的读取顺序。 select_type: 查询（select；query）类型. type：访问类型排列。The type column of EXPLAIN output describes how tables are joined. possible_keys：哪些索引可以使用。 key：哪些索引被实际使用。 ref：表之间的引用。 rows：每张表有多少行被优化器查询。 Extra：包含不适合在其他列中显示但十分重要的额外信息。 EXPLAIN输出字段 # 官方解释\nid # id：表的读取和加载顺序。\n值有以下三种情况：\nid相同，执行顺序由上至下。 id不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行。 id相同不同，同时存在。永远是id大的优先级最高，id相等的时候顺序执行。 select_type # select_type：查询（select；query）类型.，主要是用于区别，普通查询、联合查询、子查询等的复杂查询。\nSIMPLE：简单的SELECT查询，查询中不包含子查询或者UNION 。 PRIMARY：查询中如果包含任何复杂的子部分，最外层查询则被标记为PRIMARY。 SUBQUERY：在SELECT或者WHERE子句中包含了子查询。 DERIVED：在FROM子句中包含的子查询被标记为DERIVED(衍生)，MySQL会递归执行这些子查询，把结果放在临时表中。 UNION：如果第二个SELECT出现在UNION之后，则被标记为UNION；若UNION包含在FROM子句的子查询中，外层SELECT将被标记为DERIVED。 UNION RESULT：从UNION表获取结果的SELECT。 type # type：访问类型排列。\n从最好到最差依次是：system\u0026gt;const\u0026gt;eq_ref\u0026gt;ref\u0026gt;range\u0026gt;index\u0026gt;ALL。除了ALL没有用到索引，其他级别都用到索引了。\n一般来说，得保证查询至少达到range级别，最好达到ref。\nsystem：表只有一行记录（等于系统表），这是const类型的特例，平时不会出现，这个也可以忽略不计。\nconst：表示通过索引一次就找到了，const用于比较primary key或者unique索引。因为只匹配一行数据，所以很快。如将主键置于where列表中，MySQL就能将该查询转化为一个常量。\neq_ref：唯一性索引扫描，读取本表中和关联表表中的每行组合成的一行，查出来只有一条记录。除 了 system 和 const 类型之外, 这是最好的联接类型。\nref：非唯一性索引扫描，返回本表和关联表某个值匹配的所有行，查出来有多条记录。\nrange：只检索给定范围的行，一般就是在WHERE语句中出现了BETWEEN、\u0026lt; \u0026gt;、in等的查询。这种范围扫描索引比全表扫描要好，因为它只需要开始于索引树的某一点，而结束于另一点，不用扫描全部索引。\nindex：Full Index Scan，全索引扫描，index和ALL的区别为index类型只遍历索引树。也就是说虽然ALL和index都是读全表，但是index是从索引中读的，ALL是从磁盘中读取的。\nALL：Full Table Scan，没有用到索引，全表扫描。\npossible_keys 和 key # possible_keys：显示可能应用在这张表中的索引，一个或者多个。查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询实际使用。\nkey：实际使用的索引。如果为NULL，则没有使用索引。查询中如果使用了覆盖索引，则该索引仅仅出现在key列表中。\nkey_len\nkey_len：表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度。key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的。在不损失精度的情况下，长度越短越好。\nkey_len计算规则： https://blog.csdn.net/qq_34930488/article/details/102931490\nmysql\u0026gt; desc pms_category; +---------------+------------+------+-----+---------+----------------+ | Field | Type | Null | Key | Default | Extra | +---------------+------------+------+-----+---------+----------------+ | cat_id | bigint(20) | NO | PRI | NULL | auto_increment | | name | char(50) | YES | | NULL | | | parent_cid | bigint(20) | YES | | NULL | | | cat_level | int(11) | YES | | NULL | | | show_status | tinyint(4) | YES | | NULL | | | sort | int(11) | YES | | NULL | | | icon | char(255) | YES | | NULL | | | product_unit | char(50) | YES | | NULL | | | product_count | int(11) | YES | | NULL | | +---------------+------------+------+-----+---------+----------------+ 9 rows in set (0.00 sec) mysql\u0026gt; explain select cat_id from pms_category where cat_id between 10 and 20 \\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: pms_category partitions: NULL type: range possible_keys: PRIMARY key: PRIMARY # 用到了主键索引，通过查看表结构知道，cat_id是bigint类型，占用8个字节 key_len: 8 # 这里只用到了cat_id主键索引，所以长度就是8！ ref: NULL rows: 11 filtered: 100.00 Extra: Using where; Using index 1 row in set, 1 warning (0.00 sec) ref # ref：显示索引的哪一列被使用了，如果可能的话，是一个常数。哪些列或常量被用于查找索引列上的值。\nrows # rows：根据表统计信息及索引选用情况，大致估算出找到所需的记录需要读取的行数。\nExtra # Extra：包含不适合在其他列中显示但十分重要的额外信息。\nUsing filesort：说明MySQL会对数据使用一个外部的索引排序，而不是按照表内的索引顺序进行读取。MySQL中无法利用索引完成的排序操作成为\u0026quot;文件内排序\u0026quot;。 # 排序没有使用索引 mysql\u0026gt; explain select name from pms_category where name=\u0026#39;Tangs\u0026#39; order by cat_level \\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: pms_category partitions: NULL type: ref possible_keys: idx_name_parentCid_catLevel key: idx_name_parentCid_catLevel key_len: 201 ref: const rows: 1 filtered: 100.00 Extra: Using where; Using index; Using filesort 1 row in set, 1 warning (0.00 sec) #~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ ~ # 排序使用到了索引 mysql\u0026gt; explain select name from pms_category where name=\u0026#39;Tangs\u0026#39; order by parent_cid,cat_level\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: pms_category partitions: NULL type: ref possible_keys: idx_name_parentCid_catLevel key: idx_name_parentCid_catLevel key_len: 201 ref: const rows: 1 filtered: 100.00 Extra: Using where; Using index 1 row in set, 1 warning (0.00 sec) Using temporary：使用了临时表保存中间结果，MySQL在対查询结果排序时使用了临时表。常见于排序order by和分组查询group by。临时表対系统性能损耗很大。\nUsing index：表示相应的SELECT操作中使用了覆盖索引，避免访问了表的数据行，效率不错！如果同时出现Using where，表示索引被用来执行索引键值的查找；如果没有同时出现Using where，表明索引用来读取数据而非执行查找动作。\n# 覆盖索引 # 就是select的数据列只用从索引中就能够取得，不必从数据表中读取，换句话说查询列要被所使用的索引覆盖。 # 注意：如果要使用覆盖索引，一定不能写SELECT *，要写出具体的字段。 mysql\u0026gt; explain select cat_id from pms_category \\G; *************************** 1. row *************************** id: 1 select_type: SIMPLE table: pms_category partitions: NULL type: index possible_keys: NULL key: PRIMARY key_len: 8 ref: NULL rows: 1425 filtered: 100.00 Extra: Using index # select的数据列只用从索引中就能够取得，不必从数据表中读取 1 row in set, 1 warning (0.00 sec) Using where：表明使用了WHERE过滤。 Using join buffer：使用了连接缓存。 impossible where：WHERE子句的值总是false，不能用来获取任何元组。 mysql\u0026gt; explain select name from pms_category where name = \u0026#39;zs\u0026#39; and name = \u0026#39;ls\u0026#39;\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: NULL partitions: NULL type: NULL possible_keys: NULL key: NULL key_len: NULL ref: NULL rows: NULL filtered: NULL Extra: Impossible WHERE # 不可能字段同时查到两个名字 1 row in set, 1 warning (0.00 sec) Show Profile # Show Profile是什么？\nShow Profile：MySQL提供可以用来分析当前会话中语句执行的资源消耗情况。可以用于SQL的调优的测量。默认情况下，参数处于关闭状态，并保存最近15次的运行结果。\n分析步骤\n1、是否支持，看看当前的MySQL版本是否支持。\n# 查看Show Profile功能是否开启 mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;profiling\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | profiling | OFF | +---------------+-------+ 1 row in set (0.00 sec) 2、开启Show Profile功能，默认是关闭的，使用前需要开启。\n# 开启Show Profile功能 mysql\u0026gt; SET profiling=ON; Query OK, 0 rows affected, 1 warning (0.00 sec) 3、运行SQL\nSELECT * FROM `emp` GROUP BY `id`%10 LIMIT 150000; SELECT * FROM `emp` GROUP BY `id`%20 ORDER BY 5; 4、查看结果，执行SHOW PROFILES;\nDuration：持续时间。\nmysql\u0026gt; SHOW PROFILES; +----------+------------+---------------------------------------------------+ | Query_ID | Duration | Query | +----------+------------+---------------------------------------------------+ | 1 | 0.00156100 | SHOW VARIABLES LIKE \u0026#39;profiling\u0026#39; | | 2 | 0.56296725 | SELECT * FROM `emp` GROUP BY `id`%10 LIMIT 150000 | | 3 | 0.52105825 | SELECT * FROM `emp` GROUP BY `id`%10 LIMIT 150000 | | 4 | 0.51279775 | SELECT * FROM `emp` GROUP BY `id`%20 ORDER BY 5 | +----------+------------+---------------------------------------------------+ 4 rows in set, 1 warning (0.00 sec) 5、诊断SQL，SHOW PROFILE cpu,block io FOR QUERY Query_ID;\n# 这里的3是第四步中的Query_ID。 # 可以在SHOW PROFILE中看到一条SQL中完整的生命周期。 mysql\u0026gt; SHOW PROFILE cpu,block io FOR QUERY 3; +----------------------+----------+----------+------------+--------------+---------------+ | Status | Duration | CPU_user | CPU_system | Block_ops_in | Block_ops_out | +----------------------+----------+----------+------------+--------------+---------------+ | starting | 0.000097 | 0.000090 | 0.000002 | 0 | 0 | | checking permissions | 0.000010 | 0.000009 | 0.000000 | 0 | 0 | | Opening tables | 0.000039 | 0.000058 | 0.000000 | 0 | 0 | | init | 0.000046 | 0.000046 | 0.000000 | 0 | 0 | | System lock | 0.000011 | 0.000000 | 0.000000 | 0 | 0 | | optimizing | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | | statistics | 0.000023 | 0.000037 | 0.000000 | 0 | 0 | | preparing | 0.000014 | 0.000000 | 0.000000 | 0 | 0 | | Creating tmp table | 0.000041 | 0.000053 | 0.000000 | 0 | 0 | | Sorting result | 0.000005 | 0.000000 | 0.000000 | 0 | 0 | | executing | 0.000003 | 0.000000 | 0.000000 | 0 | 0 | | Sending data | 0.520620 | 0.516267 | 0.000000 | 0 | 0 | | Creating sort index | 0.000060 | 0.000051 | 0.000000 | 0 | 0 | | end | 0.000006 | 0.000000 | 0.000000 | 0 | 0 | | query end | 0.000011 | 0.000000 | 0.000000 | 0 | 0 | | removing tmp table | 0.000006 | 0.000000 | 0.000000 | 0 | 0 | | query end | 0.000004 | 0.000000 | 0.000000 | 0 | 0 | | closing tables | 0.000009 | 0.000000 | 0.000000 | 0 | 0 | | freeing items | 0.000032 | 0.000064 | 0.000000 | 0 | 0 | | cleaning up | 0.000019 | 0.000000 | 0.000000 | 0 | 0 | +----------------------+----------+----------+------------+--------------+---------------+ 20 rows in set, 1 warning (0.00 sec) Show Profile查询参数备注：\nALL：显示所有的开销信息。 BLOCK IO：显示块IO相关开销（通用）。 CONTEXT SWITCHES：上下文切换相关开销。 CPU：显示CPU相关开销信息（通用）。 IPC：显示发送和接收相关开销信息。 MEMORY：显示内存相关开销信息。 PAGE FAULTS：显示页面错误相关开销信息。 SOURCE：显示和Source_function。 SWAPS：显示交换次数相关开销的信息。 6、Show Profile查询列表，日常开发需要注意的结论：\nconverting HEAP to MyISAM：查询结果太大，内存都不够用了，往磁盘上搬了。 Creating tmp table：创建临时表（拷贝数据到临时表，用完再删除），非常耗费数据库性能。 Copying to tmp table on disk：把内存中的临时表复制到磁盘，危险！！！ locked：死锁。 打开慢查询日志 # 慢查询日志是什么？\nMySQL的慢查询日志是MySQL提供的一种日志记录，它用来记录在MySQL中响应时间超过阈值的语句，具体指运行时间超过long_query_time值的SQL，则会被记录到慢查询日志中。 long_query_time的默认值为10，意思是运行10秒以上的语句。 由慢查询日志来查看哪些SQL超出了我们的最大忍耐时间值，比如一条SQL执行超过5秒钟，我们就算慢SQL，希望能收集超过5秒钟的SQL，结合之前explain进行全面分析。 特别说明\n默认情况下，MySQL数据库没有开启慢查询日志，需要我们手动来设置这个参数。\n当然，如果不是调优需要的话，一般不建议启动该参数，因为开启慢查询日志会或多或少带来一定的性能影响。慢查询日志支持将日志记录写入文件。\n查看慢查询日志是否开以及如何开启\n查看慢查询日志是否开启：SHOW VARIABLES LIKE '%slow_query_log%';。\n开启慢查询日志：SET GLOBAL slow_query_log = 1;。使用该方法开启MySQL的慢查询日志只对当前数据库生效，如果MySQL重启后会失效。\n# 1、查看慢查询日志是否开启 mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;%slow_query_log%\u0026#39;; +---------------------+--------------------------------------+ | Variable_name | Value | +---------------------+--------------------------------------+ | slow_query_log | OFF | | slow_query_log_file | /var/lib/mysql/1dcb5644392c-slow.log | +---------------------+--------------------------------------+ 2 rows in set (0.01 sec) # 2、开启慢查询日志 mysql\u0026gt; SET GLOBAL slow_query_log = 1; Query OK, 0 rows affected (0.00 sec) 如果要使慢查询日志永久开启，需要修改my.cnf文件，在[mysqld]下增加修改参数。\n# cnf [mysqld] # 这个是开启慢查询。注意ON需要大写 slow_query_log=ON # 这个是存储慢查询的日志文件。这个文件不存在的话，需要自己创建 slow_query_log_file=/var/lib/mysql/slow.log 开启了慢查询日志后，什么样的SQL才会被记录到慢查询日志里面呢？\n这个是由参数long_query_time控制的，默认情况下long_query_time的值为10秒。\nMySQL中查看long_query_time的时间：SHOW VARIABLES LIKE 'long_query_time%';。\n# 查看long_query_time 默认是10秒 # 只有SQL的执行时间\u0026gt;10才会被记录 mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;long_query_time%\u0026#39;; +-----------------+-----------+ | Variable_name | Value | +-----------------+-----------+ | long_query_time | 10.000000 | +-----------------+-----------+ 1 row in set (0.00 sec) 修改long_query_time的时间，需要在my.cnf修改配置文件\n[mysqld] # 这个是设置慢查询的时间，我设置的为1秒 long_query_time=1 查新慢查询日志的总记录条数：SHOW GLOBAL STATUS LIKE '%Slow_queries%';。\nmysql\u0026gt; SHOW GLOBAL STATUS LIKE \u0026#39;%Slow_queries%\u0026#39;; +---------------+-------+ | Variable_name | Value | +---------------+-------+ | Slow_queries | 3 | +---------------+-------+ 1 row in set (0.00 sec) 案例 # 模拟慢查询\nSELECT SLEEP(10)，comments from article where id=1; begin; select sleep(10); SHOW VARIABLES LIKE \u0026#39;%slow_query_log%\u0026#39;; commit; 日志分析工具 # 日志分析工具mysqldumpslow：在生产环境中，如果要手工分析日志，查找、分析SQL，显然是个体力活，MySQL提供了日志分析工具mysqldumpslow。\n# 1、mysqldumpslow --help 来查看mysqldumpslow的帮助信息 root@1dcb5644392c:/usr/bin# mysqldumpslow --help Usage: mysqldumpslow [ OPTS... ] [ LOGS... ] Parse and summarize the MySQL slow query log. Options are --verbose verbose --debug debug --help write this text to standard output -v verbose -d debug -s ORDER what to sort by (al, at, ar, c, l, r, t), \u0026#39;at\u0026#39; is default # 按照何种方式排序 al: average lock time # 平均锁定时间 ar: average rows sent # 平均返回记录数 at: average query time # 平均查询时间 c: count # 访问次数 l: lock time # 锁定时间 r: rows sent # 返回记录 t: query time # 查询时间 -r reverse the sort order (largest last instead of first) -t NUM just show the top n queries # 返回前面多少条记录 -a don\u0026#39;t abstract all numbers to N and strings to \u0026#39;S\u0026#39; -n NUM abstract numbers with at least n digits within names -g PATTERN grep: only consider stmts that include this string -h HOSTNAME hostname of db server for *-slow.log filename (can be wildcard), default is \u0026#39;*\u0026#39;, i.e. match all -i NAME name of server instance (if using mysql.server startup script) -l don\u0026#39;t subtract lock time from total time # 2、 案例 # 1、得到返回记录集最多的10个SQL mysqldumpslow -s r -t 10 /var/lib/mysql/slow.log # 2、得到访问次数最多的10个SQL mysqldumpslow -s c -t 10 /var/lib/mysql/slow.log # 3、得到按照时间排序的前10条里面含有左连接的查询语句 mysqldumpslow -s t -t 10 -g \u0026#34;left join\u0026#34; /var/lib/mysql/slow.log # 4、另外建议使用这些命令时结合|和more使用，否则出现爆屏的情况 mysqldumpslow -s r -t 10 /var/lib/mysql/slow.log | more 排查慢SQL # 分析：\n1、观察，至少跑1天，看看生产的慢SQL情况。\n2、开启慢查询日志，设置阈值，比如超过5秒钟的就是慢SQL，并将它抓取出来。\n3、explain + 慢SQL分析。\n4、show Profile。\n5、运维经理 or DBA，进行MySQL数据库服务器的参数调优。\n总结（大纲）：\n1、慢查询的开启，设置阈值并捕获。\n2、explain + 慢SQL分析。\n3、show Profile查询SQL在MySQL数据库中的执行细节和生命周期情况。\n4、MySQL数据库服务器的参数调优。\n批量插入数据脚本 # 环境准备 # 1、建表SQL。\n/* 1.dept表 */ CREATE TABLE `dept` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT \u0026#39;主键\u0026#39;, `deptno` int(10) unsigned NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;部门id\u0026#39;, `dname` varchar(20) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;部门名字\u0026#39;, `loc` varchar(13) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;部门地址\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=\u0026#39;部门表\u0026#39; /* 2.emp表 */ CREATE TABLE `emp` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT COMMENT \u0026#39;主键\u0026#39;, `empno` int(10) unsigned NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;员工编号\u0026#39;, `ename` varchar(20) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;员工名字\u0026#39;, `job` varchar(9) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;职位\u0026#39;, `mgr` int(10) unsigned NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;上级编号\u0026#39;, `hiredata` date NOT NULL COMMENT \u0026#39;入职时间\u0026#39;, `sal` decimal(7,2) NOT NULL COMMENT \u0026#39;薪水\u0026#39;, `comm` decimal(7,2) NOT NULL COMMENT \u0026#39;分红\u0026#39;, `deptno` int(10) unsigned NOT NULL DEFAULT \u0026#39;0\u0026#39; COMMENT \u0026#39;部门id\u0026#39;, PRIMARY KEY (`id`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=\u0026#39;员工表\u0026#39; 2、由于开启过慢查询日志，开启了bin-log，我们就必须为function指定一个参数，否则使用函数会报错。\n# 在mysql中设置 # log_bin_trust_function_creators 默认是关闭的 需要手动开启 mysql\u0026gt; SHOW VARIABLES LIKE \u0026#39;log_bin_trust_function_creators\u0026#39;; +---------------------------------+-------+ | Variable_name | Value | +---------------------------------+-------+ | log_bin_trust_function_creators | OFF | +---------------------------------+-------+ 1 row in set (0.00 sec) mysql\u0026gt; SET GLOBAL log_bin_trust_function_creators=1; Query OK, 0 rows affected (0.00 sec) 上述修改方式MySQL重启后会失效，在my.cnf配置文件下修改永久有效。\n[mysqld] log_bin_trust_function_creators=ON 创建函数 # # 1、函数：随机产生字符串 DELIMITER $$ CREATE FUNCTION rand_string(n INT) RETURNS VARCHAR(255) BEGIN DECLARE chars_str VARCHAR(100) DEFAULT \u0026#39;abcdefghijklmnopqrstuvwsyzABCDEFGHIJKLMNOPQRSTUVWXYZ\u0026#39;; DECLARE return_str VARCHAR(255) DEFAULT \u0026#39;\u0026#39;; DECLARE i INT DEFAULT 0; WHILE i \u0026lt; n DO SET return_str = CONCAT(return_str,SUBSTRING(chars_str,FLOOR(1+RAND()*52),1)); SET i = i + 1; END WHILE; RETURN return_str; END $$ # 2、函数：随机产生部门编号 DELIMITER $$ CREATE FUNCTION rand_num() RETURNS INT(5) BEGIN DECLARE i INT DEFAULT 0; SET i = FLOOR(100 + RAND() * 10); RETURN i; END $$ 创建存储过程 # # 1、函数：向dept表批量插入 DELIMITER $$ CREATE PROCEDURE insert_dept(IN START INT(10),IN max_num INT(10)) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO dept(deptno,dname,loc) VALUES((START + i),rand_string(10),rand_string(8)); UNTIL i = max_num END REPEAT; COMMIT; END $$ # 2、函数：向emp表批量插入 DELIMITER $$ CREATE PROCEDURE insert_emp(IN START INT(10),IN max_num INT(10)) BEGIN DECLARE i INT DEFAULT 0; SET autocommit = 0; REPEAT SET i = i + 1; INSERT INTO emp(empno,ename,job,mgr,hiredata,sal,comm,deptno) VALUES((START + i),rand_string(6),\u0026#39;SALESMAN\u0026#39;,0001,CURDATE(),2000,400,rand_num()); UNTIL i = max_num END REPEAT; COMMIT; END $$ 调用存储过程 # # 1、调用存储过程向dept表插入10个部门。 DELIMITER ; CALL insert_dept(100,10); # 2、调用存储过程向emp表插入50万条数据。 DELIMITER ; CALL insert_emp(100001,500000); 结果 # mysql\u0026gt; select count(*) from dept; +----------+ | count(*) | +----------+ | 10 | +----------+ 1 row in set (0.00 sec) mysql\u0026gt; mysql\u0026gt; select count(*) from emp; +----------+ | count(*) | +----------+ | 500000 | +----------+ 1 row in set (0.07 sec) "},{"id":6,"href":"/middleware/docs/mysql/init/install/","title":"初始化","section":"开始篇","content":" MySQL环境 # 环境安装 # # 查看Linux服务器上是否安装过MySQL rpm -qa | grep -i mysql # 查询出所有mysql依赖包 # 1、拉取镜像 docker pull mysql:5.7 # 2、创建实例并启动 docker run -p 3306:3306 --name mysql \\ -v /root/mysql/log:/var/log/mysql \\ -v /root/mysql/data:/var/lib/mysql \\ -v /root/mysql/conf:/etc/mysql \\ -e MYSQL_ROOT_PASSWORD=qwer \\ -d mysql:5.7 # conf # 配置文件!!! # 4、重启mysql容器 docker restart mysql # 5、进入到mysql容器 docker exec -it mysql /bin/bash # 6、查看修改的配置文件 cat /etc/mysql/my.conf 安装位置 # Docker容器就是一个小型的Linux环境，进入到MySQL容器中。\ndocker exec -it mysql /bin/bash Linux环境下MySQL的安装目录。\n路径 解释 /var/lib/mysql MySQL数据库文件存放位置 /usr/share/mysql 错误消息和字符集文件配置 /usr/bin 客户端程序和脚本 /etc/init.d/mysql 启停脚本相关 修改字符集 # # determine which charset/collations are available SHOW CHARSET; SHOW COLLATION; # check charset SHOW VARIABLES LIKE \u0026#39;%character%\u0026#39;; SHOW VARIABLES LIKE \u0026#39;%collation%\u0026#39;; # cnf) [mysqld] character-set-server=utf8mb4 collation-server=utf8mb4_general_ci # check database/table charset:检查数据库/表的创建信息，注意这里可以在下一步中的命令改，就是它不是最早创建时的信息，未来的命令可以改变这个输出结果。 SHOW CREATE DATABASE databasename; SHOW CREATE TABLE tablename; # change the database/table charset：改变数据库/表的字符 ALTER DATABASE databasename CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; ALTER TABLE tablename CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci; # set when create database/table：可以在创建数据库/表的时候指定字符 CREATE DATABASE new_db CHARACTER SET utf8mb4 DEFAULT COLLATE utf8mb4_general_ci; CREATE TABLE new_table (id INT) CHARSET utf8mb4 COLLATE utf8mb4_general_ci; # so, if you have special character that can\u0026#39;t save to mysql, maybe you should use utf8mb4 and utf8mb4_general_ci MySQL5.7配置文件位置是/etc/my.cnf或者/etc/mysql/my.cnf，如果字符集不是utf-8直接进入配置文件修改即可。\n[client] #mysqlde utf8字符集默认为3位的，不支持emoji表情及部分不常见的汉字，故推荐使用utf8mb4 default-character-set=utf8mb4 [mysql] default-character-set=utf8mb4 [mysqld] #设置client**连接**mysql时的字符集,防止乱码 init_connect=\u0026#39;SET collation_connection = utf8mb4_general_ci\u0026#39; init_connect=\u0026#39;SET NAMES utf8mb4\u0026#39; #数据库默认字符集 character-set-server=utf8mb4 #数据库字符集对应一些排序等规则，注意要和character-set-server对应 collation-server=utf8mb4_general_ci # 跳过mysql程序起动时的字符参数设置 ，使用服务器端字符集设置 skip-character-set-client-handshake # 禁止MySQL对外部连接进行DNS解析，使用这一选项可以消除MySQL进行DNS解析的时间。但需要注意，如果开启该选项，则所有远程主机连接授权都要使用IP地址方式，否则MySQL将无法正常处理连接请求！ skip-name-resolve # 数据库错误日志文件 log-error = /var/log/mysql/error.log 注意：安装MySQL完毕之后，第一件事就是修改字符集编码。\n配置文件 # MySQL配置文件讲解：https://gist.github.com/zput/9609dbbbbe2aa1f2f516f62c699fd682\n# 查询错误日志的位置 show variables like \u0026#39;log_error\u0026#39;; show variables like \u0026#39;general_log_file\u0026#39;; show variables like \u0026#39;slow_query_log_file\u0026#39;; 1、错误日志log-error：默认是关闭的，记录严重的警告和错误信息，每次启动和关闭的详细信息等。\n# my,cnf # 数据库错误日志文件 log-error = /var/log/mysql/error.log 2、查询日志log：默认关闭，记录查询的sql语句，如果开启会降低MySQL整体的性能，因为记录日志需要消耗系统资源。\n# my,cnf # 慢查询sql日志设置 slow_query_log = 1 slow_query_log_file = slow.log 3、二进制日志log-bin：主从复制。\n# my,cnf # 开启mysql binlog功能 log-bin=mysql-bin 4、数据文件。\nfrm文件：存放表结构。 myd文件：存放表数据。 myi文件：存放表索引。 # frm文件来存储表结构 # ibd文件来存储表索引和表数据 -rw-r----- 1 mysql mysql 8988 Jun 25 09:31 pms_category.frm -rw-r----- 1 mysql mysql 245760 Jul 21 10:01 pms_category.ibd MySQL5.7的Innodb存储引擎可将所有数据存放于ibdata*的共享表空间，也可将每张表存放于独立的.ibd文件的独立表空间。 共享表空间以及独立表空间都是针对数据的存储方式而言的。\n共享表空间: 某一个数据库的所有的表数据，索引文件全部放在一个文件中，默认这个共享表空间的文件路径在data目录下。 默认的文件名为:ibdata1 初始化为10M。 独立表空间: 每一个表都将会生成以独立的文件方式来进行存储，每一个表都有一个.frm表描述文件，还有一个.ibd文件。 其中这个文件包括了单独一个表的数据内容以及索引内容，默认情况下它的存储位置也是在表的位置之中。在配置文件my.cnf中设置： innodb_file_per_table。 MySQL逻辑架构 # Connectors：指的是不同语言中与SQL的交互。\nConnection Pool：管理缓冲用户连接，线程处理等需要缓存的需求。MySQL数据库的连接层。\nManagement Serveices \u0026amp; Utilities：系统管理和控制工具。备份、安全、复制、集群等等。。\nSQL Interface：接受用户的SQL命令，并且返回用户需要查询的结果。\nParser：SQL语句解析器。\nOptimizer：查询优化器，SQL语句在查询之前会使用查询优化器对查询进行优化。就是优化客户端请求query，根据客户端请求的 query 语句，和数据库中的一些统计信息，在一系列算法的基础上进行分析，得出一个最优的策略，告诉后面的程序如何取得这个 query 语句的结果。For Example： select uid,name from user where gender = 1;这个select 查询先根据where 语句进行选取，而不是先将表全部查询出来以后再进行gender过滤；然后根据uid和name进行属性投影，而不是将属性全部取出以后再进行过滤。最后将这两个查询条件联接起来生成最终查询结果。\nCaches \u0026amp; Buffers：查询缓存。\nPluggable Storage Engines：存储引擎接口。MySQL区别于其他数据库的最重要的特点就是其插件式的表存储引擎(注意：存储引擎是基于表的，而不是数据库)。\nFile System：数据落地到磁盘上，就是文件的存储。\nMySQL数据库和其他数据库相比，MySQL有点与众不同，主要体现在存储引擎的架构上，插件式的存储引擎架构将查询处理和其他的系统任务以及数据的存储提取相分离。这种架构可以根据业务的需求和实际需求选择合适的存储引擎。\n逻辑架构分层\n连接层：最上层是一些客户端和连接服务，包含本地sock通信和大多数基于客户端/服务端工具实现的类似于tcp/ip的通信。主要完成一些类似于连接处理、授权认证、及相关的安全方案。在该层上引入了线程池的概念，为通过认证安全接入的客户端提供线程。同样在该层上可以实现基于SSL的安全链接。服务器也会为安全接入的每个客户端验证它所具有的操作权限。\n服务层：MySQL的核心服务功能层，该层是MySQL的核心，包括查询缓存，解析器，解析树，预处理器，查询优化器。主要进行查询解析、分析、查询缓存、内置函数、存储过程、触发器、视图等，select操作会先检查是否命中查询缓存，命中则直接返回缓存数据，否则解析查询并创建对应的解析树。\n引擎层：存储引擎层，存储引擎真正的负责了MySQL中数据的存储和提取，服务器通过API与存储引擎进行通信。不同的存储引擎具有的功能不同，这样我们可以根据自己的实际需要进行选取。\n存储层：数据存储层，主要是将数据存储在运行于裸设备的文件系统之上，并完成与存储引擎的交互。\n存储引擎 # show engines;命令查看MySQL5.7支持的存储引擎。\nmysql\u0026gt; show engines; show variables like 'default_storage_engine%';查看当前数据库正在使用的存储引擎。\nmysql\u0026gt; show variables like \u0026#39;default_storage_engine%\u0026#39;; +------------------------+--------+ | Variable_name | Value | +------------------------+--------+ | default_storage_engine | InnoDB | +------------------------+--------+ 1 row in set (0.01 sec) InnoDB和MyISAM对比\n对比项 MyISAM InnoDB 主外键 不支持 支持 事务 不支持 支持 行表锁 表锁，即使操作一条记录也会锁住整张表，不适合高并发操作 行锁，操作时只锁某一行，不对其他行有影响，适合高并发操作 缓存 只缓存索引，不缓存真实数据 不仅缓存索引还要缓存真实数据，対内存要求较高，而且内存大小対性能有决定性影响 表空间 小 大 关注点 性能 事务 默认安装 Y Y 主从复制 # 复制基本原理 # MySQL复制过程分为三步：\nMaster将改变记录到二进制日志(Binary Log)。这些记录过程叫做二进制日志事件，Binary Log Events； Slave将Master的Binary Log Events拷贝到它的中继日志(Replay Log); Slave重做中继日志中的事件，将改变应用到自己的数据库中。MySQL复制是异步且串行化的。 复制基本原则 # 每个Slave只有一个Master。 每个Slave只能有一个唯一的服务器ID。 每个Master可以有多个Salve。 docker-compose # github\n# 拉起Master和Slave $ docker-compose -p mysql-repl up # 连接Master $ docker exec -it mysql-repl_mysql-master_1 mysql -u root -p # 连接Slave $ docker exec -it mysql-repl_mysql-slave_1 mysql -u root -p 创建Replication用户 # 到Master上创建Replication用户：\nmysql\u0026gt; CREATE USER \u0026#39;repl\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;; mysql\u0026gt; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;repl\u0026#39;@\u0026#39;%\u0026#39;; 将Slave和Master关联 # 到Slave上把自己和Master关联起来：\n并且CHANGE MASTER TO语句有所不同，使用的是Master的Service Name以及容器内端口3306： CHANGE MASTER TO MASTER_HOST=\u0026#39;mysql-master\u0026#39;, MASTER_PORT=3306, MASTER_USER=\u0026#39;repl\u0026#39;, MASTER_PASSWORD=\u0026#39;password\u0026#39;, GET_MASTER_PUBLIC_KEY=1, MASTER_AUTO_POSITION=1; 一主一从配置 # 1、基本要求：Master和Slave的MySQL服务器版本一致且后台以服务运行。\n# 创建mysql-slave1实例 docker run -p 3307:3306 --name mysql-slave1 \\ -v /root/mysql-slave1/log:/var/log/mysql \\ -v /root/mysql-slave1/data:/var/lib/mysql \\ -v /root/mysql-slave1/conf:/etc/mysql \\ -e MYSQL_ROOT_PASSWORD=333 \\ -d mysql:5.7 2、主从配置都是配在[mysqld]节点下，都是小写\n# Master配置 [mysqld] server-id=1 # 必须 log-bin=/var/lib/mysql/mysql-bin # 必须 read-only=0 binlog-ignore-db=mysql # Slave配置 [mysqld] server-id=2 # 必须 log-bin=/var/lib/mysql/mysql-bin 3、Master配置\n# * TO \u0026#39;username\u0026#39;@\u0026#39;从机IP地址\u0026#39; IDENTIFIED BY \u0026#39;password\u0026#39;; mysql\u0026gt; GRANT REPLICATION SLAVE ON *.* TO \u0026#39;zhangsan\u0026#39;@\u0026#39;172.18.0.3\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;; Query OK, 0 rows affected, 1 warning (0.01 sec) # 2、刷新命令 mysql\u0026gt; FLUSH PRIVILEGES; Query OK, 0 rows affected (0.00 sec) # 3、记录下File和Position # 每次配从机的时候都要SHOW MASTER STATUS;查看最新的File和Position mysql\u0026gt; SHOW MASTER STATUS; +------------------+----------+--------------+------------------+-------------------+ | File | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set | +------------------+----------+--------------+------------------+-------------------+ | mysql-bin.000001 | 602 | | mysql | | +------------------+----------+--------------+------------------+-------------------+ 1 row in set (0.00 sec) 4、Slave从机配置\nCHANGE MASTER TO MASTER_HOST=\u0026#39;172.18.0.4\u0026#39;, MASTER_USER=\u0026#39;zhangsan\u0026#39;, MASTER_PASSWORD=\u0026#39;123456\u0026#39;, MASTER_LOG_FILE=\u0026#39;mysql-bin.File的编号\u0026#39;, MASTER_LOG_POS=Position的最新值; # 1、使用用户名密码登录进Master mysql\u0026gt; CHANGE MASTER TO MASTER_HOST=\u0026#39;172.18.0.4\u0026#39;, -\u0026gt; MASTER_USER=\u0026#39;zhangsan\u0026#39;, -\u0026gt; MASTER_PASSWORD=\u0026#39;123456\u0026#39;, -\u0026gt; MASTER_LOG_FILE=\u0026#39;mysql-bin.000001\u0026#39;, -\u0026gt; MASTER_LOG_POS=602; Query OK, 0 rows affected, 2 warnings (0.02 sec) # 2、开启Slave从机的复制 mysql\u0026gt; START SLAVE; Query OK, 0 rows affected (0.00 sec) # 3、查看Slave状态 # Slave_IO_Running 和 Slave_SQL_Running 必须同时为Yes 说明主从复制配置成功！ mysql\u0026gt; SHOW SLAVE STATUS\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event # Slave待命状态 Master_Host: 172.18.0.4 Master_User: zhangsan Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 602 Relay_Log_File: b030ad25d5fe-relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 602 Relay_Log_Space: 534 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: bd047557-b20c-11ea-9961-0242ac120002 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) 5、测试主从复制\n# Master创建数据库 mysql\u0026gt; create database test_replication; Query OK, 1 row affected (0.01 sec) # Slave查询数据库 mysql\u0026gt; show databases; +--------------------+ | Database | +--------------------+ | information_schema | | mysql | | performance_schema | | sys | | test_replication | +--------------------+ 5 rows in set (0.00 sec) 6、停止主从复制功能\n# 1、停止Slave mysql\u0026gt; STOP SLAVE; Query OK, 0 rows affected (0.00 sec) # 2、重新配置主从 # MASTER_LOG_FILE 和 MASTER_LOG_POS一定要根据最新的数据来配 mysql\u0026gt; CHANGE MASTER TO MASTER_HOST=\u0026#39;172.18.0.4\u0026#39;, -\u0026gt; MASTER_USER=\u0026#39;zhangsan\u0026#39;, -\u0026gt; MASTER_PASSWORD=\u0026#39;123456\u0026#39;, -\u0026gt; MASTER_LOG_FILE=\u0026#39;mysql-bin.000001\u0026#39;, -\u0026gt; MASTER_LOG_POS=797; Query OK, 0 rows affected, 2 warnings (0.01 sec) mysql\u0026gt; START SLAVE; Query OK, 0 rows affected (0.00 sec) mysql\u0026gt; SHOW SLAVE STATUS\\G *************************** 1. row *************************** Slave_IO_State: Waiting for master to send event Master_Host: 172.18.0.4 Master_User: zhangsan Master_Port: 3306 Connect_Retry: 60 Master_Log_File: mysql-bin.000001 Read_Master_Log_Pos: 797 Relay_Log_File: b030ad25d5fe-relay-bin.000002 Relay_Log_Pos: 320 Relay_Master_Log_File: mysql-bin.000001 Slave_IO_Running: Yes Slave_SQL_Running: Yes Replicate_Do_DB: Replicate_Ignore_DB: Replicate_Do_Table: Replicate_Ignore_Table: Replicate_Wild_Do_Table: Replicate_Wild_Ignore_Table: Last_Errno: 0 Last_Error: Skip_Counter: 0 Exec_Master_Log_Pos: 797 Relay_Log_Space: 534 Until_Condition: None Until_Log_File: Until_Log_Pos: 0 Master_SSL_Allowed: No Master_SSL_CA_File: Master_SSL_CA_Path: Master_SSL_Cert: Master_SSL_Cipher: Master_SSL_Key: Seconds_Behind_Master: 0 Master_SSL_Verify_Server_Cert: No Last_IO_Errno: 0 Last_IO_Error: Last_SQL_Errno: 0 Last_SQL_Error: Replicate_Ignore_Server_Ids: Master_Server_Id: 1 Master_UUID: bd047557-b20c-11ea-9961-0242ac120002 Master_Info_File: /var/lib/mysql/master.info SQL_Delay: 0 SQL_Remaining_Delay: NULL Slave_SQL_Running_State: Slave has read all relay log; waiting for more updates Master_Retry_Count: 86400 Master_Bind: Last_IO_Error_Timestamp: Last_SQL_Error_Timestamp: Master_SSL_Crl: Master_SSL_Crlpath: Retrieved_Gtid_Set: Executed_Gtid_Set: Auto_Position: 0 Replicate_Rewrite_DB: Channel_Name: Master_TLS_Version: 1 row in set (0.00 sec) 如何分库分表 # 想像成是对一张表进行拆分\n垂直：就是把一些列进行拆分，一部分列移到另一个表。 水平：就是把记录(数据)进行拆分,列的结构不变。\n垂直拆分 # 垂直分库 # 根据业务不同与微服务类似单独服务对应单独库\n垂直分表 # 垂直分表是基于数据库中的”列”进行，某个表字段较多，可以新建一张扩展表，将不经常用或字段长度较大的字段拆分出去到扩展表中。在字段很多的情况下（例如一个大表有100多个字段），通过”大表拆小表”，更便于开发与维护，也能避免跨页问题，MySQL底层是通过数据页存储的，一条记录占用空间过大会导致跨页，造成额外的性能开销。另外数据库以行为单位将数据加载到内存中，这样表中字段长度较短且访问频率较高，内存能加载更多的数据，命中率更高，减少了磁盘IO，从而提升了数据库性能。\n拆分字段的操作建议在数据库设计阶段就做好。如果是在发展过程中拆分，则需要改写以前的查询语句，会额外带来一定的成本和风险，建议谨慎\n水平拆分 # 水平拆分（根据表内数据内在的逻辑关系，将同一个表按不同的条件分散到多个数据库或多个表中，每个表中只包含一部分数据，从而使得单个表的数据量变小，达到分布式的效果。）\n优缺点： 优点： 不存在单库数据量过大、高并发的性能瓶颈，提升系统稳定性和负载能力 应用端改造较小，不需要拆分业务模块 “冷热数据分离”实现方案 缺点： 跨分片事务难以保证 跨分片的复杂查询如join关联查询 数据多次扩展难度和维护量极大 分库分表会遗留的问题 # 事务问题 # 跨表join，聚合查询order by，group by等问题 # 将原本处于mysql执行的跨表查询以及聚合查询等操作提前到网关层进行聚合。比如说现在你有这样一条SQL: select * from tableXX order by create_time desc limit 0,10; 则会从512张表中每张表都获取10条数据，然后再网关层就会出现512*10条数据，然后重新排序聚合提取10条数据返回给应用。带来的就是性能响应时间增加。\n数据倾斜问题 # 分库分表下的主键id问题 # 分库分表下的hash数据到每个表，会存在两种情况，\n一种是数据库自增id， 一种是分布式全局共用一处生成主键id。\n先说数据库自增id， 会导致我们刚才提到的假设我们现在要聚合查询，这样可能导致会出现512条id一致的数据，这样前端应用就会出现困扰。因为id是必须唯一的才能保证我们获取数据，那么我们不使用自增id，\n我们必须通过每条数据的某个值能够确定该行唯一数据，并使512张表的主键id都不一致但是有序，为什么需要补充有序？\n主键id不一致大家都知道防止冲突。 是因为我们如果使用Innodb数据存储引擎的话底层是红黑树，那么对于连续存储的key值可以有效减少随机访问次数和IO次数提升我们查询的性能，达到每次读取page页可以预读取。\n接下来说如何实现分布式全局主键id的几种方式：\nSequence ID 数据库自增长序列或字段，最常见的方式。由数据库维护，数据库唯一。 优点： 简单，代码方便，性能可以接受。 数字ID天然排序，对分页或者需要排序的结果很有帮助。 缺点： 不同数据库语法和实现不同，数据库迁移的时候或多数据库版本支持的时候需要处理。 在单个数据库或读写分离或一主多从的情况下，只有一个主库可以生成。有单点故障的风险。 在性能达不到要求的情况下，比较难于扩展。 如果遇见多个系统需要合并或者涉及到数据迁移会相当痛苦。 分表分库的时候会有麻烦。 优化方案： 针对主库单点，如果有多个Master库，则每个Master库设置的起始数字不一样，步长一样，可以是Master的个数。 比如：Master1 生成的是 1，4，7，10，Master2生成的是2,5,8,11 Master3生成的是 3,6,9,12。这样就可以有效生成集群中的唯一ID，也可以大大降低ID生成数据库操作的负载。\nUUID 常见的方式,128位。可以利用数据库也可以利用程序生成，一般来说全球唯一。 优点： 简单，代码方便。 全球唯一，在遇见数据迁移，系统数据合并，或者数据库变更等情况下，可以从容应对。 缺点： 没有排序，无法保证趋势递增。 UUID往往是使用字符串存储，查询的效率比较低。 存储空间比较大，如果是海量数据库，就需要考虑存储量的问题。 传输数据量大 不可读。 优化方案： 为了解决UUID不可读，可以使用UUID to Int64的方法。\nGUID GUID：是微软对UUID这个标准的实现。UUID还有其它各种实现，不止GUID一种。优缺点同UUID。\nCOMB COMB（combine）型是数据库特有的一种设计思想，组合的方式，保留UniqueIdentifier的前10个字节，用后6个字节表示GUID生成的时间（DateTime），将时间信息与UniqueIdentifier组合起来，在保留UniqueIdentifier的唯一性的同时增加了有序性，以此来提高索引效率。 优点： 解决UUID无序的问题，在其主键生成方式中提供了Comb算法(combined guid/timestamp)。保留GUID的10个字节，用另6个字节表示GUID生成的时间(DateTime)。 性能优于UUID。\nTwitter的snowflake算法 使用41bit作为毫秒数，10bit作为机器的ID（5个bit是数据中心，5个bit的机器ID），12bit作为毫秒内的流水号（意味着每个节点在每毫秒可以产生 4096 个 ID），最后还有一个符号位，永远是0。snowflake算法可以根据自身项目的需要进行一定的修改。比如估算未来的数据中心个数，每个数据中心的机器数以及统一毫秒可以能的并发数来调整在算法中所需要的bit数。 优点： 不依赖于数据库，灵活方便，且性能优于数据库。 ID按照时间在单机上是递增的。 缺点： 在单机上是递增的，但是由于涉及到分布式环境，每台机器上的时钟不可能完全同步，也许有时候也会出现不是全局递增的情况。\n业务上涨，伸缩性问题 # "},{"id":7,"href":"/middleware/docs/mysql/business_facing/innodb_index/","title":"索引","section":"面向业务篇","content":" 索引 # 索引简介 # 索引是什么？\nA database index is a data structure that improves the speed of operations in a table MySQL官方对索引的定义为：索引（INDEX）是帮助MySQL对表高效操作的数据结构。\n从而可以获得索引的本质：索引是排好序的快速查找数据结构。\n索引的目的在于提高查询效率，可以类比字典的目录。如果要查mysql这个这个单词，我们肯定要先定位到m字母，然后从上往下找y字母，再找剩下的sql。如果没有索引，那么可能需要a---z，这样全字典扫描，如果我想找Java开头的单词呢？如果我想找Oracle开头的单词呢？？？\n重点：索引会影响到MySQL查找(WHERE的查询条件)和排序(ORDER BY)两大功能！\n除了数据本身之外，数据库还维护着一个满足特定查找算法的数据结构，这些数据结构以某种方式指向数据，这样就可以在这些数据结构的基础上实现高级查找算法，这种数据结构就是索引。\n一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储在磁盘上。\n# Linux下查看磁盘空间命令 df -h [root@Ringo ~]# df -h Filesystem Size Used Avail Use% Mounted on /dev/vda1 40G 16G 23G 41% / devtmpfs 911M 0 911M 0% /dev tmpfs 920M 0 920M 0% /dev/shm tmpfs 920M 480K 920M 1% /run tmpfs 920M 0 920M 0% /sys/fs/cgroup overlay 40G 16G 23G 41% 我们平时所说的索引，如果没有特别指明，都是指B树（多路搜索树，并不一定是二叉的）结构组织的索引。其中聚集索引，次要索引，覆盖索引，复合索引，前缀索引，唯一索引默认都是使用B+树索引，统称索引。当然，除了B+树这种数据结构的索引之外，还有哈希索引（Hash Index）等。\n索引的优势和劣势\n优势：\n查找：类似大学图书馆的书目索引，提高数据检索的效率，降低数据库的IO成本。 排序：通过索引対数据进行排序，降低数据排序的成本，降低了CPU的消耗。 劣势：\n实际上索引也是一张表，该表保存了主键与索引字段，并指向实体表的记录，所以索引列也是要占用空间的。 虽然索引大大提高了查询速度，但是同时会降低表的更新速度，例如对表频繁的进行INSERT、UPDATE和DELETE。因为更新表的时候，MySQL不仅要保存数据，还要保存一下索引文件每次更新添加的索引列的字段，都会调整因为更新所带来的键值变化后的索引信息。 索引只是提高效率的一个因素，如果MySQL有大数据量的表，就需要花时间研究建立最优秀的索引。 索引分类 # 索引分类：\n单值索引：一个索引只包含单个列，一个表可以有多个单列索引。 唯一索引：索引列的值必须唯一，但是允许空值。 复合索引：一个索引包含多个字段。 建议：一张表建的索引最好不要超过5个！\n/* 基本语法 */ /* 1、创建索引 [UNIQUE]可以省略*/ /* 如果只写一个字段就是单值索引，写多个字段就是复合索引 */ CREATE [UNIQUE] INDEX indexName ON tabName(columnName(length)); /* 2、删除索引 */ DROP INDEX [indexName] ON tabName; /* 3、查看索引 */ /* 加上\\G就可以以列的形式查看了 不加\\G就是以表的形式查看 */ SHOW INDEX FROM tabName \\G; 使用ALTER命令来为数据表添加索引\n/* 1、该语句添加一个主键，这意味着索引值必须是唯一的，并且不能为NULL */ ALTER TABLE tabName ADD PRIMARY KEY(column_list); /* 2、该语句创建索引的键值必须是唯一的(除了NULL之外，NULL可能会出现多次) */ ALTER TABLE tabName ADD UNIQUE indexName(column_list); /* 3、该语句创建普通索引，索引值可以出现多次 */ ALTER TABLE tabName ADD INDEX indexName(column_list); /* 4、该语句指定了索引为FULLTEXT，用于全文检索 */ ALTER TABLE tabName ADD FULLTEXT indexName(column_list); 索引数据结构 # 索引数据结构：\nBTree索引。 Hash索引。 Full-text全文索引。 R-Tree索引。 BTree索引检索原理：\n哪些情况需要建索引 # 主键: 如果有主键，默认会使用主键作为聚簇索引的索引键（key）； 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键（key）； 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键（key）； where/order by: 频繁作为查询条件的字段应该创建索引。 查询中统计或者分组字段（group by也和索引有关）。 查询中排序的字段，排序字段若通过索引去访问将大大提高排序速度。 查询中与其他表关联的字段，外键关系建立索引。 那些情况不要建索引 # 记录太少的表。 经常增删改的表。 频繁更新的字段不适合创建索引。 Where条件里用不到的字段不创建索引。 假如一个表有10万行记录，有一个字段A只有true和false两种值，并且每个值的分布概率大约为50%，那么对A字段建索引一般不会提高数据库的查询速度。索引的选择性是指索引列中不同值的数目与表中记录数的比。如果一个表中有2000条记录，表索引列有1980个不同的值，那么这个索引的选择性就是1980/2000=0.99。一个索引的选择性越接近于1，这个索引的效率就越高。 索引分析 # 单表索引分析\u0026ndash;\u0026gt;范围之后的索引会失效 # 数据准备 # DROP TABLE IF EXISTS `article`; CREATE TABLE IF NOT EXISTS `article`( `id` INT(10) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT COMMENT \u0026#39;主键\u0026#39;, `author_id` INT(10) UNSIGNED NOT NULL COMMENT \u0026#39;作者id\u0026#39;, `category_id` INT(10) UNSIGNED NOT NULL COMMENT \u0026#39;分类id\u0026#39;, `views` INT(10) UNSIGNED NOT NULL COMMENT \u0026#39;被查看的次数\u0026#39;, `comments` INT(10) UNSIGNED NOT NULL COMMENT \u0026#39;回帖的备注\u0026#39;, `title` VARCHAR(255) NOT NULL COMMENT \u0026#39;标题\u0026#39;, `content` VARCHAR(255) NOT NULL COMMENT \u0026#39;正文内容\u0026#39; ) COMMENT \u0026#39;文章\u0026#39;; INSERT INTO `article`(`author_id`, `category_id`, `views`, `comments`, `title`, `content`) VALUES(1,1,1,1,\u0026#39;1\u0026#39;,\u0026#39;1\u0026#39;); INSERT INTO `article`(`author_id`, `category_id`, `views`, `comments`, `title`, `content`) VALUES(2,2,2,2,\u0026#39;2\u0026#39;,\u0026#39;2\u0026#39;); INSERT INTO `article`(`author_id`, `category_id`, `views`, `comments`, `title`, `content`) VALUES(3,3,3,3,\u0026#39;3\u0026#39;,\u0026#39;3\u0026#39;); INSERT INTO `article`(`author_id`, `category_id`, `views`, `comments`, `title`, `content`) VALUES(1,1,3,3,\u0026#39;3\u0026#39;,\u0026#39;3\u0026#39;); INSERT INTO `article`(`author_id`, `category_id`, `views`, `comments`, `title`, `content`) VALUES(1,1,4,4,\u0026#39;4\u0026#39;,\u0026#39;4\u0026#39;); 案例：查询category_id为1且comments大于1的情况下，views最多的article_id。\n编写SQL语句并查看SQL执行计划。 # # 1、sql语句 SELECT id,author_id FROM article WHERE category_id = 1 AND comments \u0026gt; 1 ORDER BY views DESC LIMIT 1; # 2、sql执行计划 mysql\u0026gt; EXPLAIN SELECT id,author_id FROM article WHERE category_id = 1 AND comments \u0026gt; 1 ORDER BY views DESC LIMIT 1\\G *************************** 1. row *************************** id: 1 select_type: SIMPLE table: article partitions: NULL type: ALL possible_keys: NULL key: NULL key_len: NULL ref: NULL rows: 5 filtered: 20.00 Extra: Using where; Using filesort # 产生了文件内排序，需要优化SQL 1 row in set, 1 warning (0.00 sec) 创建索引idx_article_ccv。 # CREATE INDEX idx_article_ccv ON article(category_id,comments,views); mysql\u0026gt; show index from article; +---------+------------+-----------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+ | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | +---------+------------+-----------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+ | article | 0 | PRIMARY | 1 | id | A | 5 | NULL | NULL | | BTREE | | | | article | 1 | idx_article_ccv | 1 | category_id | A | 3 | NULL | NULL | | BTREE | | | | article | 1 | idx_article_ccv | 2 | comments | A | 5 | NULL | NULL | | BTREE | | | | article | 1 | idx_article_ccv | 3 | views | A | 5 | NULL | NULL | | BTREE | | | +---------+------------+-----------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+ 4 rows in set (0.00 sec) 查看现在SQL语句的执行计划。 # mysql\u0026gt; EXPLAIN SELECT id,author_id FROM article WHERE category_id = 1 AND comments \u0026gt; 1 ORDER BY views DESC LIMIT 1; +----+-------------+---------+------------+-------+-----------------+-----------------+---------+------+------+----------+---------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+---------+------------+-------+-----------------+-----------------+---------+------+------+----------+---------------------------------------+ | 1 | SIMPLE | article | NULL | range | idx_article_ccv | idx_article_ccv | 8 | NULL | 2 | 100.00 | Using index condition; Using filesort | +----+-------------+---------+------------+-------+-----------------+-----------------+---------+------+------+----------+---------------------------------------+ 1 row in set, 1 warning (0.00 sec) 我们发现，创建符合索引idx_article_ccv之后，虽然解决了全表扫描的问题，但是在order by排序的时候没有用到索引，MySQL居然还是用的Using filesort，为什么？\n我们试试把SQL修改为SELECT id,author_id FROM article WHERE category_id = 1 AND comments = 1 ORDER BY views DESC LIMIT 1;看看SQL的执行计划。 # mysql\u0026gt; EXPLAIN SELECT id,author_id FROM article WHERE category_id = 1 AND comments = 1 ORDER BY views DESC LIMIT 1; +----+-------------+---------+------------+------+-----------------+-----------------+---------+-------------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+---------+------------+------+-----------------+-----------------+---------+-------------+------+----------+-------------+ | 1 | SIMPLE | article | NULL | ref | idx_article_ccv | idx_article_ccv | 8 | const,const | 1 | 100.00 | Using where | +----+-------------+---------+------------+------+-----------------+-----------------+---------+-------------+------+----------+-------------+ 1 row in set, 1 warning (0.00 sec) 推论：当comments \u0026gt; 1的时候order by排序views字段索引就用不上，但是当comments = 1的时候order by排序views字段索引就可以用上！！！所以，范围之后的索引会失效。\n我们现在知道范围之后的索引会失效，原来的索引idx_article_ccv最后一个字段views会失效，那么我们如果删除这个索引，创建idx_article_cv索引呢？？？？ # drop index idx_article_ccv on article; /* 创建索引 idx_article_cv */ CREATE INDEX idx_article_cv ON article(category_id,views); show index from article; 查看当前的索引\nmysql\u0026gt; show index from article; +---------+------------+----------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+ | Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | +---------+------------+----------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+ | article | 0 | PRIMARY | 1 | id | A | 5 | NULL | NULL | | BTREE | | | | article | 1 | idx_article_cv | 1 | category_id | A | 3 | NULL | NULL | | BTREE | | | | article | 1 | idx_article_cv | 2 | views | A | 5 | NULL | NULL | | BTREE | | | +---------+------------+----------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+ 3 rows in set (0.00 sec) 当前索引是idx_article_cv，来看一下SQL执行计划。 # mysql\u0026gt; EXPLAIN SELECT id,author_id FROM article WHERE category_id = 1 AND comments \u0026gt; 1 ORDER BY views DESC LIMIT 1; +----+-------------+---------+------------+-------+----------------+----------------+---------+------+------+----------+------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+---------+------------+-------+----------------+----------------+---------+------+------+----------+------------------------------------+ | 1 | SIMPLE | article | NULL | range | idx_article_cv | idx_article_cv | 4 | NULL | 3 | 33.33 | Using index condition; Using where | +----+-------------+---------+------------+-------+----------------+----------------+---------+------+------+----------+------------------------------------+ 1 row in set, 1 warning (0.00 sec) 两表索引分析\u0026mdash;\u0026gt;左连接将索引创建在右表上更合适 # 数据准备 # DROP TABLE IF EXISTS `class`; DROP TABLE IF EXISTS `book`; CREATE TABLE IF NOT EXISTS `class`( `id` INT(10) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT COMMENT \u0026#39;主键\u0026#39;, `card` INT(10) UNSIGNED NOT NULL COMMENT \u0026#39;分类\u0026#39; ) COMMENT \u0026#39;商品类别\u0026#39;; CREATE TABLE IF NOT EXISTS `book`( `bookid` INT(10) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT COMMENT \u0026#39;主键\u0026#39;, `card` INT(10) UNSIGNED NOT NULL COMMENT \u0026#39;分类\u0026#39; ) COMMENT \u0026#39;书籍\u0026#39;; 两表连接查询的SQL执行计划\n1、不创建索引的情况下，SQL的执行计划。\nmysql\u0026gt; EXPLAIN select * from book left join class on book.card = class.card; +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+ | 1 | SIMPLE | book | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | NULL | | 1 | SIMPLE | class | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | Using where; Using join buffer (Block Nested Loop) | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+ 2 rows in set, 1 warning (0.00 sec) book和class两张表都是没有使用索引，全表扫描，那么如果进行优化，索引是创建在book表还是创建在class表呢？下面进行大胆的尝试！\n左表(book表)创建索引。 # 创建索引idx_book_card\n/* 在book表创建索引 */ CREATE INDEX idx_book_card ON book(card); 在book表中有idx_book_card索引的情况下，查看SQL执行计划\nmysql\u0026gt; EXPLAIN select * from book left join class on book.card = class.card; +----+-------------+-------+------------+-------+---------------+---------------+---------+------+------+----------+----------------------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+-------+---------------+---------------+---------+------+------+----------+----------------------------------------------------+ | 1 | SIMPLE | book | NULL | index | NULL | idx_book_card | 4 | NULL | 1 | 100.00 | Using index | | 1 | SIMPLE | class | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | Using where; Using join buffer (Block Nested Loop) | +----+-------------+-------+------------+-------+---------------+---------------+---------+------+------+----------+----------------------------------------------------+ 2 rows in set, 1 warning (0.00 sec) 删除book表的索引，右表(class表)创建索引。 # 创建索引idx_class_card\ndrop index idx_book_card on book; /* 在class表创建索引 */ CREATE INDEX idx_class_card ON class(card); 在class表中有idx_class_card索引的情况下，查看SQL执行计划\nmysql\u0026gt; EXPLAIN select * from book left join class on book.card = class.card; +----+-------------+-------+------------+------+----------------+----------------+---------+--------------------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+----------------+----------------+---------+--------------------+------+----------+-------------+ | 1 | SIMPLE | book | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | NULL | | 1 | SIMPLE | class | NULL | ref | idx_class_card | idx_class_card | 4 | zxc_test.book.card | 1 | 100.00 | Using index | +----+-------------+-------+------------+------+----------------+----------------+---------+--------------------+------+----------+-------------+ 2 rows in set, 1 warning (0.00 sec) 由此可见，左连接将索引创建在右表上更合适，右连接将索引创建在左表上更合适。\n三张表索引分析 # 数据准备 # DROP TABLE IF EXISTS `phone`; CREATE TABLE IF NOT EXISTS `phone`( `phone_id` INT(10) UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT COMMENT \u0026#39;主键\u0026#39;, `card` INT(10) UNSIGNED NOT NULL COMMENT \u0026#39;分类\u0026#39; ) COMMENT \u0026#39;手机\u0026#39;; 三表连接查询SQL优化\n不加任何索引，查看SQL执行计划。 # mysql\u0026gt; EXPLAIN select * from class left join book on class.card = book.card left join phone on book.card = phone.card; +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+ | 1 | SIMPLE | class | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | NULL | | 1 | SIMPLE | book | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | Using where; Using join buffer (Block Nested Loop) | | 1 | SIMPLE | phone | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | Using where; Using join buffer (Block Nested Loop) | +----+-------------+-------+------------+------+---------------+------+---------+------+------+----------+----------------------------------------------------+ 3 rows in set, 1 warning (0.00 sec) 根据两表查询优化的经验，左连接需要在右表上添加索引，所以尝试在book表和phone表上添加索引。 # /* 在book表创建索引 */ CREATE INDEX idx_book_card ON book(card); /* 在phone表上创建索引 */ CREATE INDEX idx_phone_card ON phone(card); 再次执行SQL的执行计划\nysql\u0026gt; /* book */ mysql\u0026gt; CREATE INDEX idx_book_card ON book(card); Query OK, 0 rows affected (0.01 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u0026gt; mysql\u0026gt; /* phone */ mysql\u0026gt; CREATE INDEX idx_phone_card ON phone(card); Query OK, 0 rows affected (0.01 sec) Records: 0 Duplicates: 0 Warnings: 0 mysql\u0026gt; mysql\u0026gt; mysql\u0026gt; EXPLAIN select * from class left join book on class.card = book.card left join phone on book.card = phone.card; +----+-------------+-------+------------+------+----------------+----------------+---------+---------------------+------+----------+-------------+ | id | select_type | table | partitions | type | possible_keys | key | key_len | ref | rows | filtered | Extra | +----+-------------+-------+------------+------+----------------+----------------+---------+---------------------+------+----------+-------------+ | 1 | SIMPLE | class | NULL | ALL | NULL | NULL | NULL | NULL | 1 | 100.00 | NULL | | 1 | SIMPLE | book | NULL | ref | idx_book_card | idx_book_card | 4 | zxc_test.class.card | 1 | 100.00 | Using index | | 1 | SIMPLE | phone | NULL | ref | idx_phone_card | idx_phone_card | 4 | zxc_test.book.card | 1 | 100.00 | Using index | +----+-------------+-------+------------+------+----------------+----------------+---------+---------------------+------+----------+-------------+ 3 rows in set, 1 warning (0.00 sec) 结论 # JOIN语句的优化：\n尽可能减少JOIN语句中的NestedLoop（嵌套循环）的总次数：永远都是小的结果集驱动大的结果集。 优先优化NestedLoop的内层循环。 保证JOIN语句中被驱动表上JOIN条件字段已经被索引。 当无法保证被驱动表的JOIN条件字段被索引且内存资源充足的前提下，不要太吝惜Join Buffer 的设置。 索引失效 # 数据准备\nCREATE TABLE `staffs`( `id` INT(10) PRIMARY KEY AUTO_INCREMENT, `name` VARCHAR(24) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;姓名\u0026#39;, `age` INT(10) NOT NULL DEFAULT 0 COMMENT \u0026#39;年龄\u0026#39;, `pos` VARCHAR(20) NOT NULL DEFAULT \u0026#39;\u0026#39; COMMENT \u0026#39;职位\u0026#39;, `add_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT \u0026#39;入职时间\u0026#39; )COMMENT \u0026#39;员工记录表\u0026#39;; INSERT INTO `staffs`(`name`,`age`,`pos`) VALUES(\u0026#39;Ringo\u0026#39;, 18, \u0026#39;manager\u0026#39;); INSERT INTO `staffs`(`name`,`age`,`pos`) VALUES(\u0026#39;张三\u0026#39;, 20, \u0026#39;dev\u0026#39;); INSERT INTO `staffs`(`name`,`age`,`pos`) VALUES(\u0026#39;李四\u0026#39;, 21, \u0026#39;dev\u0026#39;); /* 创建索引 */ CREATE INDEX idx_staffs_name_age_pos ON `staffs`(`name`,`age`,`pos`); 索引失效的情况 # 全值匹配我最爱。 最佳左前缀法则。 不在索引列上做任何操作（计算、函数、(自动or手动)类型转换），会导致索引失效而转向全表扫描。 索引中范围条件右边的字段会全部失效。 尽量使用覆盖索引（只访问索引的查询，索引列和查询列一致），减少SELECT *。 MySQL在使用!=或者\u0026lt;\u0026gt;的时候无法使用索引会导致全表扫描。 is null、is not null也无法使用索引。 like以通配符开头%abc索引失效会变成全表扫描。 字符串不加单引号索引失效。 少用or，用它来连接时会索引失效。 最佳左前缀法则 # 案例\n/* 用到了idx_staffs_name_age_pos索引中的name字段 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = \u0026#39;Ringo\u0026#39;; /* 用到了idx_staffs_name_age_pos索引中的name, age字段 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = \u0026#39;Ringo\u0026#39; AND `age` = 18; /* 用到了idx_staffs_name_age_pos索引中的name，age，pos字段 这是属于全值匹配的情况！！！*/ EXPLAIN SELECT * FROM `staffs` WHERE `name` = \u0026#39;Ringo\u0026#39; AND `age` = 18 AND `pos` = \u0026#39;manager\u0026#39;; /* 索引没用上，ALL全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `age` = 18 AND `pos` = \u0026#39;manager\u0026#39;; /* 索引没用上，ALL全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `pos` = \u0026#39;manager\u0026#39;; /* 用到了idx_staffs_name_age_pos索引中的name字段，pos字段索引失效 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = \u0026#39;Ringo\u0026#39; AND `pos` = \u0026#39;manager\u0026#39;; 概念\n最佳左前缀法则：如果索引是多字段的复合索引，要遵守最佳左前缀法则。指的是查询从索引的最左前列开始并且不跳过索引中的字段。\n口诀：带头大哥不能死，中间兄弟不能断。\n索引列上不计算 # 案例\n# 现在要查询`name` = \u0026#39;Ringo\u0026#39;的记录下面有两种方式来查询！ # 1、直接使用 字段 = 值的方式来计算 mysql\u0026gt; SELECT * FROM `staffs` WHERE `name` = \u0026#39;Ringo\u0026#39;; +----+-------+-----+---------+---------------------+ | id | name | age | pos | add_time | +----+-------+-----+---------+---------------------+ | 1 | Ringo | 18 | manager | 2020-08-03 08:30:39 | +----+-------+-----+---------+---------------------+ 1 row in set (0.00 sec) # 2、使用MySQL内置的函数 mysql\u0026gt; SELECT * FROM `staffs` WHERE LEFT(`name`, 5) = \u0026#39;Ringo\u0026#39;; +----+-------+-----+---------+---------------------+ | id | name | age | pos | add_time | +----+-------+-----+---------+---------------------+ | 1 | Ringo | 18 | manager | 2020-08-03 08:30:39 | +----+-------+-----+---------+---------------------+ 1 row in set (0.00 sec) 我们发现以上两条SQL的执行结果都是一样的，但是执行效率有没有差距呢？？？\n通过分析两条SQL的执行计划来分析性能。\n由此可见，在索引列上进行计算，会使索引失效。\n口诀：索引列上不计算。\n范围之后全失效 # 案例\n/* 用到了idx_staffs_name_age_pos索引中的name，age，pos字段 这是属于全值匹配的情况！！！*/ EXPLAIN SELECT * FROM `staffs` WHERE `name` = \u0026#39;Ringo\u0026#39; AND `age` = 18 AND `pos` = \u0026#39;manager\u0026#39;; /* 用到了idx_staffs_name_age_pos索引中的name，age字段，pos字段索引失效 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = \u0026#39;张三\u0026#39; AND `age` \u0026gt; 18 AND `pos` = \u0026#39;dev\u0026#39;; 查看上述SQL的执行计划\n由此可知，查询范围的字段使用到了索引，但是范围之后的索引字段会失效。\n口诀：范围之后全失效。\n覆盖索引尽量用 # 在写SQL的不要使用SELECT *，用什么字段就查询什么字段。\n/* 没有用到覆盖索引 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = \u0026#39;Ringo\u0026#39; AND `age` = 18 AND `pos` = \u0026#39;manager\u0026#39;; /* 用到了覆盖索引 */ EXPLAIN SELECT `name`, `age`, `pos` FROM `staffs` WHERE `name` = \u0026#39;Ringo\u0026#39; AND `age` = 18 AND `pos` = \u0026#39;manager\u0026#39;; 口诀：查询一定不用*。\n不等有时会失效 # /* 会使用到覆盖索引 */ EXPLAIN SELECT `name`, `age`, `pos` FROM `staffs` WHERE `name` != \u0026#39;Ringo\u0026#39;; /* 索引失效 全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` != \u0026#39;Ringo\u0026#39;; like百分加右边 # /* 索引失效 全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` LIKE \u0026#39;%ing%\u0026#39;; /* 索引失效 全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` LIKE \u0026#39;%ing\u0026#39;; /* 使用索引范围查询 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` LIKE \u0026#39;Rin%\u0026#39;; 口诀：like百分加右边。\n如果一定要使用%like，而且还要保证索引不失效，那么使用覆盖索引来编写SQL。\n/* 使用到了覆盖索引 */ EXPLAIN SELECT `id` FROM `staffs` WHERE `name` LIKE \u0026#39;%in%\u0026#39;; /* 使用到了覆盖索引 */ EXPLAIN SELECT `name` FROM `staffs` WHERE `name` LIKE \u0026#39;%in%\u0026#39;; /* 使用到了覆盖索引 */ EXPLAIN SELECT `age` FROM `staffs` WHERE `name` LIKE \u0026#39;%in%\u0026#39;; /* 使用到了覆盖索引 */ EXPLAIN SELECT `pos` FROM `staffs` WHERE `name` LIKE \u0026#39;%in%\u0026#39;; /* 使用到了覆盖索引 */ EXPLAIN SELECT `id`, `name` FROM `staffs` WHERE `name` LIKE \u0026#39;%in%\u0026#39;; /* 使用到了覆盖索引 */ EXPLAIN SELECT `id`, `age` FROM `staffs` WHERE `name` LIKE \u0026#39;%in%\u0026#39;; /* 使用到了覆盖索引 */ EXPLAIN SELECT `id`,`name`, `age`, `pos` FROM `staffs` WHERE `name` LIKE \u0026#39;%in\u0026#39;; /* 使用到了覆盖索引 */ EXPLAIN SELECT `id`, `name` FROM `staffs` WHERE `pos` LIKE \u0026#39;%na\u0026#39;; /* 索引失效 全表扫描 */ EXPLAIN SELECT `name`, `age`, `pos`, `add_time` FROM `staffs` WHERE `name` LIKE \u0026#39;%in\u0026#39;; 口诀：覆盖索引保两边。\n字符要加单引号 # /* 使用到了覆盖索引 */ EXPLAIN SELECT `id`, `name` FROM `staffs` WHERE `name` = \u0026#39;Ringo\u0026#39;; /* 使用到了覆盖索引 */ EXPLAIN SELECT `id`, `name` FROM `staffs` WHERE `name` = 2000; /* 索引失效 全表扫描 */ EXPLAIN SELECT * FROM `staffs` WHERE `name` = 2000; 这里name = 2000在MySQL中会发生强制类型转换，将数字转成字符串。\n口诀：字符要加单引号。\n索引相关题目 # 假设index(a,b,c)\nWhere语句 索引是否被使用 where a = 3 Y，使用到a where a = 3 and b = 5 Y，使用到a，b where a = 3 and b = 5 Y，使用到a，b，c where b = 3 或者 where b = 3 and c = 4 或者 where c = 4 N，没有用到a字段 where a = 3 and c = 5 使用到a，但是没有用到c，因为b断了 where a = 3 and b \u0026gt; 4 and c = 5 使用到a，b，但是没有用到c，因为c在范围之后 where a = 3 and b like \u0026lsquo;kk%\u0026rsquo; and c = 4 Y，a，b，c都用到 where a = 3 and b like \u0026lsquo;%kk\u0026rsquo; and c = 4 只用到a where a = 3 and b like \u0026lsquo;%kk%\u0026rsquo; and c = 4 只用到a where a = 3 and b like \u0026lsquo;k%kk%\u0026rsquo; and c = 4 Y，a，b，c都用到 面试题分析 # 数据准备\n/* 创建表 */ CREATE TABLE `test03`( `id` INT PRIMARY KEY NOT NULL AUTO_INCREMENT, `c1` CHAR(10), `c2` CHAR(10), `c3` CHAR(10), `c4` CHAR(10), `c5` CHAR(10) ); /* 插入数据 */ INSERT INTO `test03`(`c1`,`c2`,`c3`,`c4`,`c5`) VALUES(\u0026#39;a1\u0026#39;,\u0026#39;a2\u0026#39;,\u0026#39;a3\u0026#39;,\u0026#39;a4\u0026#39;,\u0026#39;a5\u0026#39;); INSERT INTO `test03`(`c1`,`c2`,`c3`,`c4`,`c5`) VALUES(\u0026#39;b1\u0026#39;,\u0026#39;b22\u0026#39;,\u0026#39;b3\u0026#39;,\u0026#39;b4\u0026#39;,\u0026#39;b5\u0026#39;); INSERT INTO `test03`(`c1`,`c2`,`c3`,`c4`,`c5`) VALUES(\u0026#39;c1\u0026#39;,\u0026#39;c2\u0026#39;,\u0026#39;c3\u0026#39;,\u0026#39;c4\u0026#39;,\u0026#39;c5\u0026#39;); INSERT INTO `test03`(`c1`,`c2`,`c3`,`c4`,`c5`) VALUES(\u0026#39;d1\u0026#39;,\u0026#39;d2\u0026#39;,\u0026#39;d3\u0026#39;,\u0026#39;d4\u0026#39;,\u0026#39;d5\u0026#39;); INSERT INTO `test03`(`c1`,`c2`,`c3`,`c4`,`c5`) VALUES(\u0026#39;e1\u0026#39;,\u0026#39;e2\u0026#39;,\u0026#39;e3\u0026#39;,\u0026#39;e4\u0026#39;,\u0026#39;e5\u0026#39;); /* 创建复合索引 */ CREATE INDEX idx_test03_c1234 ON `test03`(`c1`,`c2`,`c3`,`c4`); 题目\n/* 最好索引怎么创建的，就怎么用，按照顺序使用，避免让MySQL再自己去翻译一次 */ /* 1.全值匹配 用到索引c1 c2 c3 c4全字段 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c2` = \u0026#39;a2\u0026#39; AND `c3` = \u0026#39;a3\u0026#39; AND `c4` = \u0026#39;a4\u0026#39;; /* 2.用到索引c1 c2 c3 c4全字段 MySQL的查询优化器会优化SQL语句的顺序*/ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c2` = \u0026#39;a2\u0026#39; AND `c4` = \u0026#39;a4\u0026#39; AND `c3` = \u0026#39;a3\u0026#39;; /* 3.用到索引c1 c2 c3 c4全字段 MySQL的查询优化器会优化SQL语句的顺序*/ EXPLAIN SELECT * FROM `test03` WHERE `c4` = \u0026#39;a4\u0026#39; AND `c3` = \u0026#39;a3\u0026#39; AND `c2` = \u0026#39;a2\u0026#39; AND `c1` = \u0026#39;a1\u0026#39;; /* 4.用到索引c1 c2 c3字段，c4字段失效，范围之后全失效 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c2` = \u0026#39;a2\u0026#39; AND `c3` \u0026gt; \u0026#39;a3\u0026#39; AND `c4` = \u0026#39;a4\u0026#39;; /* 5.用到索引c1 c2 c3 c4全字段 MySQL的查询优化器会优化SQL语句的顺序*/ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c2` = \u0026#39;a2\u0026#39; AND `c4` \u0026gt; \u0026#39;a4\u0026#39; AND `c3` = \u0026#39;a3\u0026#39;; /* 6.用到了索引c1 c2 c3三个字段, c1和c2两个字段用于查找, c3字段用于排序了但是没有统计到key_len中，c4字段失效 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c2` = \u0026#39;a2\u0026#39; AND `c4` = \u0026#39;a4\u0026#39; ORDER BY `c3`; /* 7.用到了索引c1 c2 c3三个字段，c1和c2两个字段用于查找, c3字段用于排序了但是没有统计到key_len中*/ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c2` = \u0026#39;a2\u0026#39; ORDER BY `c3`; /* 8.用到了索引c1 c2两个字段，c4失效，c1和c2两个字段用于查找，c4字段排序产生了Using filesort说明排序没有用到c4字段 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c2` = \u0026#39;a2\u0026#39; ORDER BY `c4`; /* 9.用到了索引c1 c2 c3三个字段，c1用于查找，c2和c3用于排序 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c5` = \u0026#39;a5\u0026#39; ORDER BY `c2`, `c3`; /* 10.用到了c1一个字段，c1用于查找，c3和c2两个字段索引失效，产生了Using filesort */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c5` = \u0026#39;a5\u0026#39; ORDER BY `c3`, `c2`; /* 11.用到了c1 c2 c3三个字段，c1 c2用于查找，c2 c3用于排序 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c2` = \u0026#39;a2\u0026#39; ORDER BY c2, c3; /* 12.用到了c1 c2 c3三个字段，c1 c2用于查找，c2 c3用于排序 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c2` = \u0026#39;a2\u0026#39; AND `c5` = \u0026#39;a5\u0026#39; ORDER BY c2, c3; /* 13.用到了c1 c2 c3三个字段，c1 c2用于查找，c2 c3用于排序 没有产生Using filesort 因为之前c2这个字段已经确定了是\u0026#39;a2\u0026#39;了，这是一个常量，再去ORDER BY c3,c2 这时候c2已经不用排序了！ 所以没有产生Using filesort 和(10)进行对比学习！ */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c2` = \u0026#39;a2\u0026#39; AND `c5` = \u0026#39;a5\u0026#39; ORDER BY c3, c2; /* GROUP BY 表面上是叫做分组，但是分组之前必定排序。 */ /* 14.用到c1 c2 c3三个字段，c1用于查找，c2 c3用于排序，c4失效 */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c4` = \u0026#39;a4\u0026#39; GROUP BY `c2`,`c3`; /* 15.用到c1这一个字段，c4失效，c2和c3排序失效产生了Using filesort */ EXPLAIN SELECT * FROM `test03` WHERE `c1` = \u0026#39;a1\u0026#39; AND `c4` = \u0026#39;a4\u0026#39; GROUP BY `c3`,`c2`; GROUP BY基本上都需要进行排序，索引优化几乎和ORDER BY一致，但是GROUP BY会有临时表的产生。\n总结 # 索引优化的一般性建议：\n对于单值索引，尽量选择针对当前query过滤性更好的索引。 在选择复合索引的时候，当前query中过滤性最好的字段在索引字段顺序中，位置越靠前越好。 在选择复合索引的时候，尽量选择可以能够包含当前query中的where子句中更多字段的索引。 尽可能通过分析统计信息和调整query的写法来达到选择合适索引的目的。 口诀：\n带头大哥不能死。 中间兄弟不能断。 索引列上不计算。 范围之后全失效。 覆盖索引尽量用。 不等有时会失效。 like百分加右边。 字符要加单引号。 一般SQL少用or。 "},{"id":8,"href":"/middleware/docs/mysql/business_facing/innodb_lock/","title":"锁","section":"面向业务篇","content":" 表锁(偏读) # 表锁特点：\n表锁偏向MyISAM存储引擎，开销小，加锁快，无死锁，锁定粒度大，发生锁冲突的概率最高，并发度最低。 环境准备 # # 1、创建表 CREATE TABLE `mylock`( `id` INT NOT NULL PRIMARY KEY AUTO_INCREMENT, `name` VARCHAR(20) )ENGINE=MYISAM DEFAULT CHARSET=utf8 COMMENT=\u0026#39;测试表锁\u0026#39;; # 2、插入数据 INSERT INTO `mylock`(`name`) VALUES(\u0026#39;ZhangSan\u0026#39;); INSERT INTO `mylock`(`name`) VALUES(\u0026#39;LiSi\u0026#39;); INSERT INTO `mylock`(`name`) VALUES(\u0026#39;WangWu\u0026#39;); INSERT INTO `mylock`(`name`) VALUES(\u0026#39;ZhaoLiu\u0026#39;); 锁表的命令 # 1、查看数据库表锁的命令。\n# 查看数据库表锁的命令 SHOW OPEN TABLES; 2、给mylock表上读锁，给book表上写锁。\n# 给mylock表上读锁，给book表上写锁 LOCK TABLE `mylock` READ, `book` WRITE; # 查看当前表的状态 mysql\u0026gt; SHOW OPEN TABLES; +--------------------+------------------------------------------------------+--------+-------------+ | Database | Table | In_use | Name_locked | +--------------------+------------------------------------------------------+--------+-------------+ | sql_analysis | book | 1 | 0 | | sql_analysis | mylock | 1 | 0 | +--------------------+------------------------------------------------------+--------+-------------+ 3、释放表锁。\n# 释放给表添加的锁 UNLOCK TABLES; # 查看当前表的状态 mysql\u0026gt; SHOW OPEN TABLES; +--------------------+------------------------------------------------------+--------+-------------+ | Database | Table | In_use | Name_locked | +--------------------+------------------------------------------------------+--------+-------------+ | sql_analysis | book | 0 | 0 | | sql_analysis | mylock | 0 | 0 | +--------------------+------------------------------------------------------+--------+-------------+ 读锁案例 # 1、打开两个会话，SESSION1为mylock表添加读锁。\n# 为mylock表添加读锁 LOCK TABLE `mylock` READ; 2、打开两个会话，SESSION1是否可以读自己锁的表？是否可以修改自己锁的表？是否可以读其他的表？那么SESSION2呢？\n# SESSION1 # 问题1：SESSION1为mylock表加了读锁，可以读mylock表！ mysql\u0026gt; SELECT * FROM `mylock`; +----+----------+ | id | name | +----+----------+ | 1 | ZhangSan | | 2 | LiSi | | 3 | WangWu | | 4 | ZhaoLiu | +----+----------+ 4 rows in set (0.00 sec) # 问题2：SESSION1为mylock表加了读锁，不可以修改mylock表！ mysql\u0026gt; UPDATE `mylock` SET `name` = \u0026#39;abc\u0026#39; WHERE `id` = 1; ERROR 1099 (HY000): Table \u0026#39;mylock\u0026#39; was locked with a READ lock and can\u0026#39;t be updated # 问题3：SESSION1为mylock表加了读锁，不可以读其他的表！ mysql\u0026gt; SELECT * FROM `book`; ERROR 1100 (HY000): Table \u0026#39;book\u0026#39; was not locked with LOCK TABLES # SESSION2 # 问题1：SESSION1为mylock表加了读锁，SESSION2可以读mylock表！ mysql\u0026gt; SELECT * FROM `mylock`; +----+----------+ | id | name | +----+----------+ | 1 | ZhangSan | | 2 | LiSi | | 3 | WangWu | | 4 | ZhaoLiu | +----+----------+ 4 rows in set (0.00 sec) # 问题2：SESSION1为mylock表加了读锁，SESSION2修改mylock表会被阻塞，需要等待SESSION1释放mylock表！ mysql\u0026gt; UPDATE `mylock` SET `name` = \u0026#39;abc\u0026#39; WHERE `id` = 1; ^C^C -- query aborted ERROR 1317 (70100): Query execution was interrupted # 问题3：SESSION1为mylock表加了读锁，SESSION2可以读其他表！ mysql\u0026gt; SELECT * FROM `book`; +--------+------+ | bookid | card | +--------+------+ | 1 | 1 | | 7 | 4 | | 8 | 4 | | 9 | 5 | | 5 | 6 | | 17 | 6 | | 15 | 8 | +--------+------+ 24 rows in set (0.00 sec) 写锁案例 # 1、打开两个会话，SESSION1为mylock表添加写锁。\n# 为mylock表添加写锁 LOCK TABLE `mylock` WRITE; 2、打开两个会话，SESSION1是否可以读自己锁的表？是否可以修改自己锁的表？是否可以读其他的表？那么SESSION2呢？\n# SESSION1 # 问题1：SESSION1为mylock表加了写锁，可以读mylock的表！ mysql\u0026gt; SELECT * FROM `mylock`; +----+----------+ | id | name | +----+----------+ | 1 | ZhangSan | | 2 | LiSi | | 3 | WangWu | | 4 | ZhaoLiu | +----+----------+ 4 rows in set (0.00 sec) # 问题2：SESSION1为mylock表加了写锁，可以修改mylock表! mysql\u0026gt; UPDATE `mylock` SET `name` = \u0026#39;abc\u0026#39; WHERE `id` = 1; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 # 问题3：SESSION1为mylock表加了写锁，不能读其他表! mysql\u0026gt; SELECT * FROM `book`; ERROR 1100 (HY000): Table \u0026#39;book\u0026#39; was not locked with LOCK TABLES # SESSION2 # 问题1：SESSION1为mylock表加了写锁，SESSION2读mylock表会阻塞，等待SESSION1释放！ mysql\u0026gt; SELECT * FROM `mylock`; ^C^C -- query aborted ERROR 1317 (70100): Query execution was interrupted # 问题2：SESSION1为mylock表加了写锁，SESSION2读mylock表会阻塞，等待SESSION1释放！ mysql\u0026gt; UPDATE `mylock` SET `name` = \u0026#39;abc\u0026#39; WHERE `id` = 1; ^C^C -- query aborted ERROR 1317 (70100): Query execution was interrupted # 问题3：SESSION1为mylock表加了写锁，SESSION2可以读其他表！ mysql\u0026gt; SELECT * FROM `book`; +--------+------+ | bookid | card | +--------+------+ | 1 | 1 | | 7 | 4 | | 8 | 4 | | 9 | 5 | | 5 | 6 | | 17 | 6 | | 15 | 8 | +--------+------+ 24 rows in set (0.00 sec) 案例结论 # MyISAM引擎在执行查询语句SELECT之前，会自动给涉及到的所有表加读锁，在执行增删改之前，会自动给涉及的表加写锁。\nMySQL的表级锁有两种模式：\n表共享读锁（Table Read Lock）。\n表独占写锁（Table Write Lock）。\n対MyISAM表进行操作，会有以下情况：\n対MyISAM表的读操作（加读锁），不会阻塞其他线程対同一表的读操作，但是会阻塞其他线程対同一表的写操作。只有当读锁释放之后，才会执行其他线程的写操作。 対MyISAM表的写操作（加写锁），会阻塞其他线程対同一表的读和写操作，只有当写锁释放之后，才会执行其他线程的读写操作。 表锁分析 # mysql\u0026gt; SHOW STATUS LIKE \u0026#39;table%\u0026#39;; +----------------------------+-------+ | Variable_name | Value | +----------------------------+-------+ | Table_locks_immediate | 173 | | Table_locks_waited | 0 | | Table_open_cache_hits | 5 | | Table_open_cache_misses | 8 | | Table_open_cache_overflows | 0 | +----------------------------+-------+ 5 rows in set (0.00 sec) 可以通过Table_locks_immediate和Table_locks_waited状态变量来分析系统上的表锁定。具体说明如下：\nTable_locks_immediate：产生表级锁定的次数，表示可以立即获取锁的查询次数，每立即获取锁值加1。\nTable_locks_waited：出现表级锁定争用而发生等待的次数（不能立即获取锁的次数，每等待一次锁值加1），此值高则说明存在较严重的表级锁争用情况。\n此外，MyISAM的读写锁调度是写优先，这也是MyISAM不适合作为主表的引擎。因为写锁后，其他线程不能进行任何操作，大量的写操作会使查询很难得到锁，从而造成永远阻塞。\n行锁(偏写) # 行锁特点：\n偏向InnoDB存储引擎，开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度最高。 InnoDB存储引擎和MyISAM存储引擎最大不同有两点：一是支持事务，二是采用行锁。\n事务的ACID：\nAtomicity [ˌætəˈmɪsəti] 。 Consistency [kənˈsɪstənsi] 。 Isolation [ˌaɪsəˈleɪʃn]。 Durability [ˌdjʊərəˈbɪlɪti] 。 环境准备 # # 建表语句 CREATE TABLE `test_innodb_lock`( `a` INT, `b` VARCHAR(16) )ENGINE=INNODB DEFAULT CHARSET=utf8 COMMENT=\u0026#39;测试行锁\u0026#39;; # 插入数据 INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(1, \u0026#39;b2\u0026#39;); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(2, \u0026#39;3\u0026#39;); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(3, \u0026#39;4000\u0026#39;); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(4, \u0026#39;5000\u0026#39;); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(5, \u0026#39;6000\u0026#39;); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(6, \u0026#39;7000\u0026#39;); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(7, \u0026#39;8000\u0026#39;); INSERT INTO `test_innodb_lock`(`a`, `b`) VALUES(8, \u0026#39;9000\u0026#39;); # 创建索引 CREATE INDEX idx_test_a ON `test_innodb_lock`(a); CREATE INDEX idx_test_b ON `test_innodb_lock`(b); 行锁案例 # 1、开启手动提交\n打开SESSION1和SESSION2两个会话，都开启手动提交。\n# 开启MySQL数据库的手动提交 mysql\u0026gt; SET autocommit=0; Query OK, 0 rows affected (0.00 sec) 2、读几知所写\n# SESSION1 # SESSION1対test_innodb_lock表做写操作，但是没有commit。 # 执行修改SQL之后，查询一下test_innodb_lock表，发现数据被修改了。 mysql\u0026gt; UPDATE `test_innodb_lock` SET `b` = \u0026#39;88\u0026#39; WHERE `a` = 1; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 mysql\u0026gt; SELECT * FROM `test_innodb_lock`; +------+------+ | a | b | +------+------+ | 1 | 88 | | 2 | 3 | | 3 | 4000 | | 4 | 5000 | | 5 | 6000 | | 6 | 7000 | | 7 | 8000 | | 8 | 9000 | +------+------+ 8 rows in set (0.00 sec) # SESSION2 # SESSION2这时候来查询test_innodb_lock表。 # 发现SESSION2是读不到SESSION1未提交的数据的。 mysql\u0026gt; SELECT * FROM `test_innodb_lock`; +------+------+ | a | b | +------+------+ | 1 | b2 | | 2 | 3 | | 3 | 4000 | | 4 | 5000 | | 5 | 6000 | | 6 | 7000 | | 7 | 8000 | | 8 | 9000 | +------+------+ 8 rows in set (0.00 se 3、行锁两个SESSION同时対一条记录进行写操作\n# SESSION1 対test_innodb_lock表的`a`=1这一行进行写操作，但是没有commit mysql\u0026gt; UPDATE `test_innodb_lock` SET `b` = \u0026#39;99\u0026#39; WHERE `a` = 1; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 # SESSION2 也对test_innodb_lock表的`a`=1这一行进行写操作，但是发现阻塞了！！！ # 等SESSION1执行commit语句之后，SESSION2的SQL就会执行了 mysql\u0026gt; UPDATE `test_innodb_lock` SET `b` = \u0026#39;asdasd\u0026#39; WHERE `a` = 1; ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 4、行锁两个SESSION同时对不同记录进行写操作\n# SESSION1 対test_innodb_lock表的`a`=6这一行进行写操作，但是没有commit mysql\u0026gt; UPDATE `test_innodb_lock` SET `b` = \u0026#39;8976\u0026#39; WHERE `a` = 6; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 # SESSION2 対test_innodb_lock表的`a`=4这一行进行写操作，没有阻塞！！！ # SESSION1和SESSION2同时对不同的行进行写操作互不影响 mysql\u0026gt; UPDATE `test_innodb_lock` SET `b` = \u0026#39;Ringo\u0026#39; WHERE `a` = 4; Query OK, 1 row affected (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 0 索引失效行锁变表锁 # # SESSION1 执行SQL语句，没有执行commit。 # 由于`b`字段是字符串，但是没有加单引号导致索引失效 mysql\u0026gt; UPDATE `test_innodb_lock` SET `a` = 888 WHERE `b` = 8000; Query OK, 1 row affected, 1 warning (0.00 sec) Rows matched: 1 Changed: 1 Warnings: 1 # SESSION2 和SESSION1操作的并不是同一行，但是也被阻塞了？？？ # 由于SESSION1执行的SQL索引失效，导致行锁升级为表锁。 mysql\u0026gt; UPDATE `test_innodb_lock` SET `b` = \u0026#39;1314\u0026#39; WHERE `a` = 1; ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction where条件里面没有使用索引，那么就变成了全表扫描，update 语句也变成了表锁.\n间隙锁的危害 # 什么是间隙锁？\n当我们用范围条件而不是相等条件检索数据，并请求共享或者排他锁时，InnoDB会给符合条件的已有数据记录的索引项加锁，对于键值在条件范文内但并不存在的记录，叫做\u0026quot;间隙(GAP)\u0026quot;。\nInnoDB也会对这个\u0026quot;间隙\u0026quot;加锁，这种锁的机制就是所谓的\u0026quot;间隙锁\u0026quot;。\n间隙锁的危害\n因为Query执行过程中通过范围查找的话，他会锁定整个范围内所有的索引键值，即使这个键值不存在。\n间隙锁有一个比较致命的缺点，就是**当锁定一个范围的键值后，即使某些不存在的键值也会被无辜的锁定，而造成在锁定的时候无法插入锁定键值范围内的任何数据。**在某些场景下这可能会対性能造成很大的危害。\n如何锁定一行 # SELECT .....FOR UPDATE在锁定某一行后，其他写操作会被阻塞，直到锁定的行被COMMIT。\n案例结论 # InnoDB存储引擎由于实现了行级锁定，虽然在锁定机制的实现方面所带来的性能损耗可能比表级锁定会要更高一些，但是在整体并发处理能力方面要远远优于MyISAM的表级锁定的。当系统并发量较高的时候，InnoDB的整体性能和MyISAM相比就会有比较明显的优势了。\n但是，InnoDB的行级锁定同样也有其脆弱的一面，当我们使用不当的时候，可能会让InnoDB的整体性能表现不仅不能比MyISAM高，甚至可能会更差。\n行锁分析 # mysql\u0026gt; SHOW STATUS LIKE \u0026#39;innodb_row_lock%\u0026#39;; +-------------------------------+--------+ | Variable_name | Value | +-------------------------------+--------+ | Innodb_row_lock_current_waits | 0 | | Innodb_row_lock_time | 124150 | | Innodb_row_lock_time_avg | 31037 | | Innodb_row_lock_time_max | 51004 | | Innodb_row_lock_waits | 4 | +-------------------------------+--------+ 5 rows in set (0.00 sec) 対各个状态量的说明如下：\nInnodb_row_lock_current_waits：当前正在等待锁定的数量。 Innodb_row_lock_time：从系统启动到现在锁定总时间长度（重要）。 Innodb_row_lock_time_avg：每次等待所花的平均时间（重要）。 Innodb_row_lock_time_max：从系统启动到现在等待最长的一次所花的时间。 Innodb_row_lock_waits：系统启动后到现在总共等待的次数（重要）。 尤其是当等待次数很高，而且每次等待时长也不小的时候，我们就需要分析系统中为什么会有如此多的等待，然后根据分析结果着手制定优化策略。\n死锁 # InnoDB 实现了以下两种类型的行锁：\n共享锁（S）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。 排他锁（X）：允许获得排他锁的事务更新数据，阻止其他事务取得相同数据集的共享读锁和排他写锁。 为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB 还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁：\n意向共享锁（IS）：事务打算给数据行加行共享锁，事务在给一个数据行加共享锁前必须先取得该表的 IS 锁。 意向排他锁（IX）：事务打算给数据行加行排他锁，事务在给一个数据行加排他锁前必须先取得该表的 IX 锁。 （如果一个事务请求的锁模式与当前的锁兼容， InnoDB 就将请求的锁授予该事务； 反之， 如果两者不兼容，该事务就要等待锁释放。）\nInnoDB加锁方法： 对于普通SELECT语句，InnoDB 不会加任何锁； 对于 UPDATE、 DELETE 和 INSERT 语句， InnoDB会自动给涉及数据集加排他锁（X)； 事务可以通过以下语句显式给记录集加共享锁或排他锁： 共享锁（S）：SELECT * FROM table_name WHERE ... LOCK IN SHARE MODE。 其他 session 仍然可以查询记录，并也可以对该记录加 share mode 的共享锁。但是如果当前事务需要对该记录进行更新操作，则很有可能造成死锁。 排他锁（X)：SELECT * FROM table_name WHERE ... FOR UPDATE。其他 session 可以查询该记录，但是不能对该记录加共享锁或排他锁，而是等待获得锁 意向锁是 InnoDB 自动加的， 不需用户干预。 死锁有三种情况\n第一种情况，用户1访问表A(锁住A表)，然后访问表B，用户2，访问表B(锁住B),然后访问表A,这样用户1等待用户2释放表B上的锁，用户2等待用户1释放表A上的锁！ 解决方法： 主要由于逻辑程序bug产生，在事务的执行顺序上，如果有多个资源，保证代码执行资源顺序的一致性，比如事务A和事务B都是更新A表后然后执行B表，那么都按这个顺序执行，如操作A和B两张表时，总是按先A后B的顺序处理， 必须同时锁定两个资源时，要保证在任何时刻都应该按照相同的顺序来锁定资源。\n第二种情况，事务a查询一条记录，然后准备更新这条记录，事务b进行同样的操作，但由于速度过快，事务a没来来得及commit,那么事务a查询获得这条记录的共享锁，并且事务a准备更新获得排他锁，此时b同样获得该条记录的共享锁，事务a等待事务b释放锁，进行更新，同样事务b等待事务a执行完进行更新，产生死锁！场景如下，如果一个按钮没有失效设置，用户一直点，就会出现上述情况 解决方法：\n按钮点击置灰 使用悲观锁，select for update 在查询的时候就加排他锁，那么事务b就不能进行查询获得共享锁 使用乐观锁，在每条数据加版本号version字段，对所读操作的数据都不加锁，事务a和事务b第一部查出相应的版本号v，在更新前在把提交数据与数据库版本比较，例如，update set where (v+1)\u0026gt;version,如果不大于认为过期就不进行更新了 第三种情况，如果使用一条update不满足条件的sql语句，那么行锁变成了表锁， 比如索引失效情况，用了is null或者type建立索引或者使用了like %，或者的话where条件里面没有使用索引，那么就变成了全表扫描，update 语句也变成了表锁，那么这样的事务过多，造成死锁或阻塞！\nInnoDB加锁方法\n数据一致性 # 假设有A、B两个用户同时各购买一件 id=1 的商品，用户A获取到的库存量为 1000，用户B获取到的库存量也为 1000，用户A完成购买后修改该商品的库存量为 999，用户B完成购买后修改该商品的库存量为 999，此时库存量数据产生了不一致。\n有两种解决方案：\n悲观锁方案：每次获取商品时，对该商品加排他锁。也就是在用户A获取获取 id=1 的商品信息时对该行记录加锁，期间其他用户阻塞等待访问该记录。悲观锁适合写入频繁的场景。\nbegin; select * from goods where id = 1 for update; update goods set stock = stock - 1 where id = 1; commit; 乐观锁方案：每次获取商品时，不对该商品加锁。在更新数据的时候需要比较程序中的库存量与数据库中的库存量是否相等，如果相等则进行更新，反之程序重新获取库存量，再次进行比较，直到两个库存量的数值相等才进行数据更新。乐观锁适合读取频繁的场景。\n# 不加锁获取 id=1 的商品对象 select * from goods where id = 1 begin; # 更新 stock 值，这里需要注意 where 条件 “stock = cur_stock”，只有程序中获取到的库存量与数据库中的库存量相等才执行更新 update goods set stock = stock - 1 where id = 1 and stock = cur_stock; commit; 如果我们需要设计一个商城系统，该选择以上的哪种方案呢？\n查询商品的频率比下单支付的频次高，基于以上我可能会优先考虑第二种方案（当然还有其他的方案，这里只考虑以上两种方案）。\n但是第二种的话，要注意下它的先开始查询，然后开始事物，可能事物里面有许多的其他查询耗时。\n"},{"id":9,"href":"/middleware/docs/mysql/advanced/log/","title":"三种日志","section":"高级篇","content":" log # redo log # what: 物理格式日志，记录的是物理数据页面的修改的信息，其redo log是顺序写入redo log file的物理文件中去的。 why: 确保事务的持久性。 防止在发生故障的时间点，尚有脏页未写入磁盘，在重启mysql服务的时候，根据redo log进行重做，从而达到事务的持久性这一特性。 how: 什么时候产生： 事务开始之后就产生redo log，redo log的落盘并不是当事务提交时才写入的，而是在事务的执行过程中，便开始写入redo log文件中。 重做日志有一个缓存区Innodb_log_buffer，Innodb_log_buffer的默认大小为8M(这里设置的16M),Innodb存储引擎先将重做日志写入innodb_log_buffer中。 然后会通过以下三种方式将innodb日志缓冲区的日志刷新到磁盘 Master Thread 每秒一次执行刷新Innodb_log_buffer到重做日志文件。 每个事务提交时会将重做日志刷新到重做日志文件。 当重做日志缓存可用空间 少于一半时，重做日志缓存被刷新到重做日志文件 什么时候释放： 当对应事务的脏页写入到磁盘之后，redo log的使命也就完成了，重做日志占用的空间就可以重用（被覆盖）。 对应的物理文件： 默认情况下，对应的物理文件位于数据库的data目录下的ib_logfile1\u0026amp;ib_logfile2 innodb_log_group_home_dir 指定日志文件组所在的路径，默认./ ，表示在数据库的数据目录下。 innodb_log_files_in_group 指定重做日志文件组中文件的数量，默认2 关于文件的大小和数量，由一下两个参数配置 innodb_log_file_size 重做日志文件的大小。 innodb_mirrored_log_groups 指定了日志镜像文件组的数量，默认1 undo log回滚日志 # what: 逻辑格式的日志，在执行undo的时候，仅仅是将数据从逻辑上恢复至事务之前的状态，而不是从物理页面上操作实现的，这一点是不同于redo log的。 why: 保存了事务发生之前的数据的一个版本，可以用于回滚，同时可以提供多版本并发控制下的读（MVCC），也即非锁定读 how: 什么时候产生： 事务开始之前，将当前是的版本生成undo log，undo 也会产生 redo 来保证undo log的可靠性 什么时候释放： 当事务提交之后，undo log并不能立马被删除， 而是放入待清理的链表，由purge线程判断是否由其他事务在使用undo段中表的上一个事务之前的版本信息，决定是否可以清理undo log的日志空间。 对应的物理文件： MySQL5.6之前，undo表空间位于共享表空间的回滚段中，共享表空间的默认的名称是ibdata，位于数据文件目录中。 MySQL5.6之后，undo表空间可以配置成独立的文件，但是提前需要在配置文件中配置，完成数据库初始化后生效且不可改变undo log文件的个数如果初始化数据库之前没有进行相关配置，那么就无法配置成独立的表空间了。 关于MySQL5.7之后的独立undo 表空间配置参数如下 innodb_undo_directory = /data/undospace/ \u0026ndash;undo独立表空间的存放目录 innodb_undo_logs = 128 \u0026ndash;回滚段为128KB innodb_undo_tablespaces = 4 \u0026ndash;指定有4个undo log文件 如果undo使用的共享表空间，这个共享表空间中又不仅仅是存储了undo的信息，共享表空间的默认为与MySQL的数据目录下面，其属性由参数innodb_data_file_path配置。 其他： undo是在事务开始之前保存的被修改数据的一个版本，产生undo日志的时候，同样会伴随类似于保护事务持久化机制的redolog的产生。 默认情况下undo文件是保持在共享表空间的，也即ibdatafile文件中，当数据库中发生一些大的事务性操作的时候，要生成大量的undo信息，全部保存在共享表空间中的。 因此共享表空间可能会变的很大，默认情况下，也就是undo 日志使用共享表空间的时候，被“撑大”的共享表空间是不会也不能自动收缩的.mysql5.7之后的“独立undo 表空间”的配置就显得很有必要了。 binlog二进制日志 # what:\n逻辑格式的日志，可以简单认为就是执行过的事务中的sql语句。但又不完全是sql语句这么简单，而是包括了执行的sql语句（增删改）反向的信息，也就意味着delete对应着delete本身和其反向的insert；update对应着update执行前后的版本的信息；insert对应着delete和insert本身的信息。 在使用mysqlbinlog解析binlog之后一些都会真相大白。 why:\n用于复制，在主从复制中，从库利用主库上的binlog进行重播，实现主从同步。 用于数据库的基于时间点的还原。 how:\n什么时候产生： 事务提交的时候，一次性将事务中的sql语句（一个事物可能对应多个sql语句）按照一定的格式记录到binlog中。 这里与redo log很明显的差异就是redo log并不一定是在事务提交的时候刷新到磁盘，redo log是在事务开始之后就开始逐步写入磁盘。 因此对于事务的提交，即便是较大的事务，提交（commit）都是很快的，但是在开启了bin_log的情况下，对于较大事务的提交，可能会变得比较慢一些。 这是因为binlog是在事务提交的时候一次性写入的造成的，这些可以通过测试验证。 什么时候释放： binlog的默认是保持时间由参数expire_logs_days配置，也就是说对于非活动的日志文件，在生成时间超过expire_logs_days配置的天数之后，会被自动删除。 对应的物理文件： 配置文件的路径为log_bin_basename，binlog日志文件按照指定大小，当日志文件达到指定的最大的大小之后，进行滚动更新，生成新的日志文件。 对于每个binlog日志文件，通过一个统一的index文件来组织。 二进制日志的作用之一是还原数据库的，这与redo log很类似，很多人混淆过，但是两者有本质的不同 作用不同：redo log是保证事务的持久性的，是事务层面的，binlog作为还原的功能，是数据库层面的（当然也可以精确到事务层面的），虽然都有还原的意思，但是其保护数据的层次是不一样的。 内容不同：redo log是物理日志，是数据页面的修改之后的物理记录，binlog是逻辑日志，可以简单认为记录的就是sql语句 另外，两者日志产生的时间，可以释放的时间，在可释放的情况下清理机制，都是完全不同的。 恢复数据时候的效率，基于物理日志的redo log恢复数据的效率要高于语句逻辑日志的binlog MySQL通过两阶段提交过程来完成事务的一致性的，也即redo log和binlog的一致性的，理论上是先写redo log，再写binlog，两个日志都提交成功（刷入磁盘），事务才算真正的完成。\n"},{"id":10,"href":"/middleware/docs/mysql/advanced/segment_extent_page/","title":"页的结构","section":"高级篇","content":" 表空间的格式 # segment和extent是InnoDB内部用于分配管理页的逻辑结构(物理上不存在，只是逻辑上存在这样的上下级关系，物理上都是页组成)，用于分配与回收页，对于写入数据的性能至关重要。\n但这张图有所局限性，可能会产生误解：\n图中是系统表空间，因此存在rollback segment，独立表空间则没有。 leaf node segment实际是InnoDB的inode概念，一个segment可能包含最多32个碎片page、0个extent（用于小表优化），或者是非常多的extent，我猜测作者图中画了4个extent是在描述表超过32MB大小的时候一次申请4个extent。 一个extent在默认16k的page大小下，由64个page组成，page大小由UNIV_PAGE_SIZE定义，所以extent不一定由64个page组成。 表的所有行数据都存在页类型为INDEX的索引页（page）上，为了管理表空间，还需要很多其他的辅助页，例如文件管理页FSP_HDR/XDES、插入缓冲IBUF_BITMAP页、INODE页等。 页的结构 # MySQL一次IO的最小单位是页（page），也可以理解为一次原子操作都是以page为单位的，默认大小16k。刚刚列出的所有物理文件结构上都是以Page构成的，只是page内部的结构不同。\n文件管理页 # 文件管理页的页类型是FSP_HDR和XDES（extent descriptor），用于分配、管理extent和page。 FSP_HDR和XDES的唯一区别，它们两者除了FSP Header这个位置在FSP_HDR有值，在XDES中是用0填充外, 其他field都一样。 FSP Header这个field，它在XDES页中是空的(zero-filled for XDES pages) ---\u0026gt; (112); page 0(FSP_HDR页面)中FSP_HDR中有值。 FSP_HDR页都是page 0，XDES页一般出现在page 16384, 32768等固定的位置。一个FSP_HDR或者XDES页大小同样是16K。 一般情况下，每个extent都有一个占40字节的XDES entry描述维护，Innodb这里定义了一个页保存256个extent描述符, 每个区描述符所在页的偏移量都是16384(256x64, 16384, 后续随着空间文件增大，XDES页会在16384,32768等位置), 换言之管理256M，16384个page）。 FSP header: 里面最重要的信息就是四个链表头尾数据（FLST_BASE_NODE结构，FLST意思是first and last），FLST_BASE_NODE如下。 1）当一个Extent中所有page都未被使用时，挂在FSP_FREE list base node上，可以用于随后的分配； 2）有一部分page被写入的extent，挂在FREE_FRAG list base node上； 3）全满的extent，挂在FULL_FRAG list base node上； 4）归属于某个segment时候挂在FSEG list base node上。 当InnoDB写入数据的时候，会从这些链表上分配或者回收extent和page，这些extent也都是在这几个链表上移动的。 XDES entry 存储所管理的extent状态： 1）FREE（空） 2）FREE_FRAG（至少一个被占用） 3）FULL_FRAG（满） 4）归某个segment管理的信息 XDES entry还存储了每个extent内部page是否free（有空间）信息（用bitmap表示）。XDES entry组成了一个双向链表，同一种extent状态的收尾连在一起，便于管理。 INODE页 # what:\nwhy:\nhow:\nsegment是表空间管理的逻辑单位。INODE页就是用于管理segment的，每个Inode entry负责一个segment。 一个segment由32个碎片页（fragment array），FSEG_FREE、FSEG_NOT_FULL、FSEG_FULL组成，这些信息记录在Inode entry里，可以简单理解为Inode就是segment元信息的载体。 说说List base node这种数据结构, 它结构如下: FSEG_FREE、FSEG_NOT_FULL、FSEG_FULL都是属于这种结构对象 FSP_HDR/XDES页里面的FSP_FREE、FREE_FRAG、FULL_FRAG、FSEG也是。 这些链表被InnoDB使用，用于高效的管理页分配和回收。 至于碎片页上（fragment array），用于优化小表空间分配，先从全局的碎片分配Page，当fragment array填满（32个槽位）时，之后每次分配一个完整的Extent，如果表大于32MB，则一次分配4个extent。\nMySQL的数据是按照B+ tree聚簇索引（clustered index）组织数据的，每个B+ tree使用两个segment来管理page，分别是leaf node segment（叶子节点segment）和non-leaf node segment（非叶子节点segment）。\n这两个segment的Inode entry地址记录在B+ tree的root page中FSEG_HEADER里面，而root page又被分配在non-leaf segment第一个碎片页上（fragment array）。 INDEX数据索引页 # 索引（index）用于快速定位数据，对于InnoDB来说，主键和非主键都是索引，一切数据都存储在INDEX索引页中，索引即数据，数据即索引。 clustered index将数据按照索引的顺序存储。通常来讲，索引和数据在一起，找到了索引也就找到了数据（但不一定强求）。 unclustered index则将数据与索引分开结构，索引指向了具体的记录。索引相近的记录，在物理文件上相距可能很远。\n主键索引, 为聚簇索引。\n二级索引，就可以理解为非聚簇索引，也是一颗B+树，只不过这棵树的叶子节点是指向聚簇索引主键的，可以看做“行指针”，因此查询的时候需要“回表”。\nB+树结构特点：\n叶子节点（leaf node）存储数据，非叶子节点（non-leaf node）只是索引，这样非叶子节点就会足够的小，因此数据很“热”，便于更好的缓存。 数据和索引顺序一致，充分利用磁盘顺序IO性能普遍高于随机IO的特性。 并且有引用横向链接，支持范围查找, 可以在2-3次的IO操作内完成千万级别的表操作; 索引的结构\n聚簇和非聚簇 # what: Clustered/Unclustered Clustered Index determines the location of indexed records Typically, clustered index is one where values are data records (but not necessary) Unclustered Index cannot reorder data, does not determine data location In these indexes: value = pointer to data record why: how: 就是它的实际数据也是按照顺序排的，那么可以向上和向下查找，但是如果是非聚族的，它如果想找比它大一点点的值，你不可能从Data file里面就能找到，你必须回归到Data entries？ 使用B+树聚簇索引（B+ tree clustered index）的好处在于， 1）数据和索引顺序一致，充分利用磁盘顺序IO性能普遍高于随机IO的特性。 2）对于局部性查询也会大有裨益。 3）采用B+树，叶子节点（leaf node）存储数据，非叶子节点（non-leaf node）只是索引，这样非叶子节点就会足够的小(一个页面就能存储很多的节点，查找的时候，减少了磁盘io)，因此数据很“热”，便于更好的缓存。 4）对于覆盖索引，可以直接利用叶子节点的主键值。 二级索引，就可以理解为非聚簇索引，也是一颗B+树，只不过这棵树的叶子节点是指向聚簇索引主键的，可以看做“行指针”，因此查询的时候需要“回表”。 index # what: 主键、二级索引、行和列 B+树的**每个节点(包括叶子和非叶子节点)**都是一个INDEX索引页，其结构都是相同的； 对于聚簇索引，非叶子节点包含主键和child page number，叶子节点包含主键和具体的行； 对于非聚簇索引，也就是二级索引，非叶子节点包含二级索引和child page number，叶子节点包含二级索引和主键值。 叶子和非叶子都在index索引页，那么inode里面保存的segment信息，是不是这个segment用的所有页都能从这里找到，（先用的32个碎片页，然后也能从管理页中申请extent?） why:\nhow:\npage directory从Fil Trailer开始从后往前写，里面包含槽位slots，每个slot 2个字节，存储了某些record在该页中的物理偏移量，例如图中最后面是infimum record的offset，最前面是supremum record的offset，中间从后往前是r4，r8，r12 record的offset，一般总是每隔4-8个record添加一个slot，这样slots就等同于一个稀疏索引（sparse index），加速页内查询的办法就是通过二分查找，查询key的时间复杂度可以降为O(logN)，由于页都在内存里面，所以查询性能可控，内存访问延迟100ns左右，如果在CPU缓存中则可能更快。 Row Format # what: # row format可通过innodb_default_row_format参数指定，也可以在建表的时候指定。\nCREATE TABLE choose_row_format ( id INT, ) ENGINE=InnoDB ROW_FORMAT=DYNAMIC; REDUNDANT: 是比较老的格式， COMPACT: 5.6版本默认，COMPACT比REDUNDANT要更节省空间，大概在20%左右。 DYNAMIC: 5.7版本默认，DYNAMIC在变长存储上做了更大的空间优化，对于VARBINARY, VARCHAR, BLOB和TEXT类型更友好， COMPRESSED是压缩页。 what: # how: # row的格式在上面图中简单介绍过，由可选的两个标识+record header+body组成，具体如下。\n4）索引：序列化后存储于此，例如int类型索引主键就占用4个字节。 对于聚簇索引的叶子节点，存储行。 对于二级索引的叶子节点，存储行的主键值。 对于聚簇索引和二级索引的非叶子节点，存储child page最小的key。 上面提到的infimum和supremum中就只存字符串在行数据里。\nppt blog\n讨论innodb数据结构 # 索引最终选择B+树的原因: 1，范围查询，2，减少磁盘IO 减少IO的次数，每次读取尽可能多的数据. 因为B+树非叶子节点没有数据，可以存储更多的节点或者叫索引数据，比B树更加矮胖，可以更快的定位到叶子节点，自然就减少了磁盘IO的次数。 如果范围数据左右子树都有 B+树的数据结构特点是：数据都在叶子节点，而叶子节点又是通过指针相连，是有序链表. hash很快，但每次IO只能取一个数(范围查找不适合); 哈希索引只需要计算一次就可以获取到对应的数据，检索速度非常快。但是 Mysql 并没有采取哈希作为其底层算法，这是为什么呢？因为考虑到数据检索有一个常用手段就是范围查找，比如以下这个 SQL 语句：select \\* from user where id \\\u0026gt;3;针对以上这个语句，我们希望做的是找出 id\u0026gt;3 的数据，这是很典型的范围查找。如果使用哈希算法实现的索引，范围查找怎么做呢？一个简单的思路就是一次把所有数据找出来加载到内存，然后再在内存里筛选筛选目标范围内的数据。但是这个范围查找的方法也太笨重了，没有一点效率而言。 所以，使用哈希算法实现的索引虽然可以做到快速检索数据，但是没办法做数据高效范围查找，因此哈希索引是不适合作为 Mysql 的底层索引的数据结构。 AVL和红黑树，在大量数据的情况下，IO操作还是太多; B树每个节点内存储的是数据，因此每个节点存储的分支太少; B+节点存储的是索引+指针(引用指向下一个节点)，可以存储大量索引，同时最终 非叶子节点没有数据;数据存储在叶子节点. 并且有引用横向链接，可以在2-3次的IO操作内完成千万级别的表操作; 建议索引是是自增长数字，这样适合范围查找. "}]